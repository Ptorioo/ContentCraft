{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17BNEU7UR5Bh",
        "outputId": "6dfd70f2-3428-4177-ff05-5e708a78c3a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Restarting runtime to finalize installation ...\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# ==== Cell 1 (新): 安裝套件 ====\n",
        "# =========================\n",
        "# 目的：解決 \"numpy.dtype size changed\" 之類 ABI 衝突\n",
        "!pip -q install --upgrade pip\n",
        "\n",
        "# 這組版本彼此相容（含 CPU 版 PyTorch）\n",
        "!pip -q install \"numpy==1.26.4\" \"pandas==2.2.2\" \"scipy==1.11.4\" \"scikit-learn==1.5.1\"\n",
        "!pip -q install \"pillow==10.4.0\" \"tqdm==4.66.5\" \"regex==2024.9.11\" \"emoji==2.12.1\"\n",
        "!pip -q install \"opencv-python-headless==4.10.0.84\" \"easyocr==1.7.1\" \"openpyxl==3.1.5\"\n",
        "!pip -q install \"transformers==4.44.2\"\n",
        "!pip -q install \"torch==2.4.1\" \"torchvision==0.19.1\" \"torchaudio==2.4.1\" -q\n",
        "\n",
        "# 安裝完成後，強制重啟 Colab runtime，讓新 numpy/pandas 生效\n",
        "import os, sys, time\n",
        "print(\"Restarting runtime to finalize installation ...\")\n",
        "time.sleep(1)\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tON4t-ZwPCDo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "這是舊的～～\n",
        "# =========================\n",
        "# ==== Cell 1 (舊): 安裝套件 ====\n",
        "# =========================\n",
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip -q install transformers==4.44.2 pillow==10.4.0 pandas==2.2.2 numpy==1.26.4 scikit-learn==1.5.1 tqdm==4.66.5 regex==2024.9.11 emoji==2.12.1\n",
        "!pip -q install easyocr==1.7.1 opencv-python-headless==4.10.0.84\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoD6LdQyPH5C",
        "outputId": "5e211c4a-2c9d-42aa-9005-ab0682ea99a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# ==== Cell 2: 掛載雲端 ====\n",
        "# =========================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 設定路徑\n",
        "BASE_DIR = '/content/drive/MyDrive/project/new'\n",
        "TRAIN_CSV = f'{BASE_DIR}/with_rel_paths_train_posts.csv'\n",
        "TEST_CSV  = f'{BASE_DIR}/with_rel_paths_test_posts.csv'\n",
        "BRAND_FOLLOWERS_CSV = f'{BASE_DIR}/brand_followers.csv'\n",
        "BRAND_FOLLOWERS_XLSX = f'{BASE_DIR}/brand_followers.xlsx'\n",
        "\n",
        "IMG_TRAIN_DIR = f'{BASE_DIR}/train'\n",
        "IMG_TEST_DIR  = f'{BASE_DIR}/test'\n",
        "\n",
        "# 輸出目錄\n",
        "OUT_DIR = f'{BASE_DIR}/outputs'\n",
        "import os\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "CACHE_DIR = f'{BASE_DIR}/cache'\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "85Zx5M-6PJJI"
      },
      "outputs": [],
      "source": [
        "# ===================================\n",
        "# ==== Cell 3: 匯入、函式 ========\n",
        "# ===================================\n",
        "import os, ast, re, json, math, gc, unicodedata, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# ---- 有待調整!! ----\n",
        "K_CLUSTERS = 6\n",
        "TAU = 0.07                 # diversity 用的 softmax 溫度\n",
        "OCR_MAX_IMAGES = 1         # 每篇最多 OCR 幾張\n",
        "IMG_MAX_IMAGES = 1         # 每篇最多取幾張圖算影像嵌入\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "# ---- 函式 ----\n",
        "def safe_read_followers():\n",
        "    if os.path.exists(BRAND_FOLLOWERS_CSV):\n",
        "        df = pd.read_csv(BRAND_FOLLOWERS_CSV)\n",
        "        return df\n",
        "    elif os.path.exists(BRAND_FOLLOWERS_XLSX):\n",
        "        df = pd.read_excel(BRAND_FOLLOWERS_XLSX)\n",
        "        return df\n",
        "    else:\n",
        "        raise FileNotFoundError(\"找不到 brand_followers.csv 或 brand_followers.xlsx，請放到 MyDrive/project/new/\")\n",
        "\n",
        "def parse_rel_img_paths(cell):\n",
        "    \"\"\"\n",
        "    支援幾種常見格式：\n",
        "    - \"['a.jpg', 'b.jpg']\"\n",
        "    - 'a.jpg|b.jpg'\n",
        "    - 'a.jpg,b.jpg'\n",
        "    - 單一路徑字串\n",
        "    \"\"\"\n",
        "    if pd.isna(cell):\n",
        "        return []\n",
        "    s = str(cell).strip()\n",
        "    if s == '' or s.lower() == 'nan':\n",
        "        return []\n",
        "    # 嘗試 list 字串\n",
        "    try:\n",
        "        val = ast.literal_eval(s)\n",
        "        if isinstance(val, list):\n",
        "            return [str(x).strip() for x in val]\n",
        "    except Exception:\n",
        "        pass\n",
        "    # 以常見分隔符切\n",
        "    if '|' in s:\n",
        "        return [x.strip() for x in s.split('|') if x.strip()]\n",
        "    if ',' in s:\n",
        "        return [x.strip() for x in s.split(',') if x.strip()]\n",
        "    return [s]\n",
        "\n",
        "EMOJI_PATTERN = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
        "def count_emojis(text):\n",
        "    if not isinstance(text, str): return 0\n",
        "    return len(EMOJI_PATTERN.findall(text))\n",
        "\n",
        "def count_pattern(text, pattern):\n",
        "    if not isinstance(text, str): return 0\n",
        "    return len(re.findall(pattern, text))\n",
        "\n",
        "def has_any(text, keywords):\n",
        "    if not isinstance(text, str): return 0\n",
        "    low = text.lower()\n",
        "    return int(any(kw in low for kw in keywords))\n",
        "\n",
        "def l2_normalize(v, eps=1e-9):\n",
        "    v = np.asarray(v, dtype=np.float32)\n",
        "    n = np.linalg.norm(v) + eps\n",
        "    return v / n\n",
        "\n",
        "def cosine_sim(a, b, eps=1e-9):\n",
        "    a = a / (np.linalg.norm(a) + eps)\n",
        "    b = b / (np.linalg.norm(b) + eps)\n",
        "    return float(np.dot(a, b))\n",
        "\n",
        "def softmax(x, tau=1.0):\n",
        "    x = np.asarray(x, dtype=np.float32)\n",
        "    z = (x - np.max(x)) / max(tau, 1e-8)\n",
        "    e = np.exp(z)\n",
        "    return e / (e.sum() + 1e-9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpEu049FPK2B",
        "outputId": "879c116d-1c30-4fcf-94c2-5a61154d656e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train columns: ['type', 'sum', 'sum_pure', 'shortcode', 'time', 'ftime', 'count_like', 'count_comment', 'count_like_pure', 'count_comment_pure', 'thum', 'pic', 'pic_p', 'down_pic', 'is_video', 'brand', 'video', 'down_video', 'pinned', 'ftime_parsed'] ... total 24\n",
            "Test  columns: ['type', 'sum', 'sum_pure', 'shortcode', 'time', 'ftime', 'count_like', 'count_comment', 'count_like_pure', 'count_comment_pure', 'thum', 'pic', 'pic_p', 'down_pic', 'is_video', 'brand', 'video', 'down_video', 'pinned', 'ftime_parsed'] ... total 24\n"
          ]
        }
      ],
      "source": [
        "# ==================================\n",
        "# ==== Cell 4 讀資料 ====\n",
        "# ==================================\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "# 指定欄位名\n",
        "brand_col   = 'brand'      # IG 名稱\n",
        "caption_col = 'sum'        # 貼文文字\n",
        "time_col    = 'ftime_parsed'  # 發文時間（形如 \"2025-05-01 20:52:16\"）\n",
        "\n",
        "# 讀 followers（優先 xlsx，否則 csv）\n",
        "def load_followers_table():\n",
        "    if os.path.exists(BRAND_FOLLOWERS_XLSX):\n",
        "        df = pd.read_excel(BRAND_FOLLOWERS_XLSX)\n",
        "    elif os.path.exists(BRAND_FOLLOWERS_CSV):\n",
        "        df = pd.read_csv(BRAND_FOLLOWERS_CSV)\n",
        "    else:\n",
        "        raise FileNotFoundError(\"找不到 brand_followers.xlsx 或 brand_followers.csv，請放到 MyDrive/project/new/\")\n",
        "    required = {'brand','followers'}\n",
        "    missing = required - set(df.columns)\n",
        "    if missing:\n",
        "        raise KeyError(f\"brand_followers 檔案缺少欄位：{missing}\")\n",
        "    return df[['brand','followers']].copy()\n",
        "\n",
        "followers_df = load_followers_table()\n",
        "\n",
        "# 合併 followers\n",
        "train = train_df.merge(followers_df, on='brand', how='left')\n",
        "test  = test_df.merge(followers_df,  on='brand', how='left')\n",
        "train['followers'] = train['followers'].fillna(0.0).astype(float)\n",
        "test['followers']  = test['followers'].fillna(0.0).astype(float)\n",
        "\n",
        "# 必要欄位檢查\n",
        "required_cols = ['brand','sum','count_like','count_comment','rel_img_paths','ftime_parsed']\n",
        "for col in required_cols:\n",
        "    if col not in train.columns or col not in test.columns:\n",
        "        raise KeyError(f\"找不到必要欄位：{col}，請確認 CSV。\")\n",
        "\n",
        "# 快速看一下欄位\n",
        "print(\"Train columns:\", list(train.columns)[:20], \"... total\", len(train.columns))\n",
        "print(\"Test  columns:\", list(test.columns)[:20],  \"... total\", len(test.columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrbTB_P1PNlp",
        "outputId": "fc07673d-5ee0-4ce3-c6a6-56119724f583"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
            "/usr/local/lib/python3.12/dist-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        }
      ],
      "source": [
        "# =======================================\n",
        "# ==== Cell 5: OCR（帶快取）與讀圖工具 ====\n",
        "# =======================================\n",
        "import easyocr\n",
        "reader = easyocr.Reader(['ch_tra','en'], gpu=True)  # 中文繁體 + 英文\n",
        "\n",
        "def ocr_single_image(img_path):\n",
        "    try:\n",
        "        result = reader.readtext(img_path, detail=0, paragraph=True)\n",
        "        # 回傳合併後的文字\n",
        "        text = \" \".join([r.strip() for r in result if isinstance(r, str)])\n",
        "        return text\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def ocr_post(rel_paths, base_dir, cache_key):\n",
        "    \"\"\"\n",
        "    rel_paths: list[str] 相對路徑（或檔名）\n",
        "    base_dir : train/test 圖片資料夾\n",
        "    cache_key: 用 brand/timestamp/shortcode 等組出唯一鍵\n",
        "    \"\"\"\n",
        "    cache_file = os.path.join(CACHE_DIR, f'ocr_{cache_key}.json')\n",
        "    if os.path.exists(cache_file):\n",
        "        try:\n",
        "            with open(cache_file,'r',encoding='utf-8') as f:\n",
        "                return json.load(f).get('text','')\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    texts = []\n",
        "    cnt = 0\n",
        "    for rp in rel_paths[:OCR_MAX_IMAGES]:\n",
        "        img_path = os.path.join(base_dir, rp)\n",
        "        if not os.path.exists(img_path):\n",
        "            # 也許 rel 路徑本身就是完整檔名在 base_dir 下\n",
        "            img_path2 = os.path.join(base_dir, os.path.basename(rp))\n",
        "            if os.path.exists(img_path2):\n",
        "                img_path = img_path2\n",
        "            else:\n",
        "                continue\n",
        "        t = ocr_single_image(img_path)\n",
        "        if t: texts.append(t)\n",
        "        cnt += 1\n",
        "    final_text = \" \".join(texts).strip()\n",
        "    with open(cache_file,'w',encoding='utf-8') as f:\n",
        "        json.dump({'text':final_text}, f, ensure_ascii=False)\n",
        "    return final_text\n",
        "\n",
        "def load_image_for_clip(img_path, size_check=True):\n",
        "    try:\n",
        "        im = Image.open(img_path).convert('RGB')\n",
        "        if size_check:\n",
        "            # 避免極端大圖\n",
        "            im.thumbnail((1024,1024))\n",
        "        return im\n",
        "    except (FileNotFoundError, UnidentifiedImageError, OSError):\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBrQ-NAlPPXJ",
        "outputId": "9fcf7bff-158f-427a-a3bd-32cd15bd7fd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# ==== Cell 6 ：CLIP 模型與嵌入函式 ====\n",
        "# =========================================\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    CLIPModel, CLIPProcessor,\n",
        "    ChineseCLIPModel, ChineseCLIPProcessor,\n",
        ")\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 可選 'chinese-clip' 或 'openai-clip'\n",
        "MODEL_BACKEND = 'chinese-clip'   # 中文情境用這個\n",
        "MODEL_ID_CN   = 'OFA-Sys/chinese-clip-vit-base-patch16'\n",
        "MODEL_ID_EN   = 'openai/clip-vit-base-patch32'\n",
        "\n",
        "# 載入模型與處理器\n",
        "if MODEL_BACKEND == 'chinese-clip':\n",
        "    clip_model = ChineseCLIPModel.from_pretrained(MODEL_ID_CN).to(device)\n",
        "    clip_processor = ChineseCLIPProcessor.from_pretrained(MODEL_ID_CN)\n",
        "    PROJ_DIM = clip_model.config.projection_dim  # 一般為 512\n",
        "else:\n",
        "    clip_model = CLIPModel.from_pretrained(MODEL_ID_EN).to(device)\n",
        "    clip_processor = CLIPProcessor.from_pretrained(MODEL_ID_EN)\n",
        "    PROJ_DIM = clip_model.config.projection_dim  # 一般為 512\n",
        "\n",
        "import torch, numpy as np\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_text_clip(texts, batch_size=64, max_length=64, device_override=None, use_fp16=True):\n",
        "    \"\"\"\n",
        "    分批把 texts 送入 CLIP 文字塔做向量化，並 L2 normalize。\n",
        "    - batch_size: 建議先 64，不夠再降\n",
        "    - max_length: 限制 tokenizer 輸入長度，避免超長 OCR 文字吃光記憶體\n",
        "    - device_override: 需要時可強制 'cpu'（避免 GPU 爆）\n",
        "    - use_fp16: GPU 上用 FP16 省記憶體\n",
        "    \"\"\"\n",
        "    dev = device_override if device_override is not None else device\n",
        "    feats_all = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        chunk = texts[i:i+batch_size]\n",
        "        # 先做字串裁切（避免極端長句），再交給 tokenizer 做 token 截斷\n",
        "        chunk = [str(s)[:512] for s in chunk]\n",
        "\n",
        "        inputs = clip_processor(\n",
        "            text=chunk, return_tensors='pt',\n",
        "            padding=True, truncation=True, max_length=max_length\n",
        "        ).to(dev)\n",
        "\n",
        "        # 半精度自動混合（僅 GPU 啟用）\n",
        "        use_amp = (dev == 'cuda') and use_fp16\n",
        "        ctx = torch.cuda.amp.autocast(enabled=use_amp, dtype=torch.float16) if use_amp else torch.autocast('cpu', enabled=False)\n",
        "        with ctx:\n",
        "            feats = clip_model.get_text_features(**inputs)  # [n, d]\n",
        "\n",
        "        arr = feats.detach().cpu().numpy()\n",
        "        arr = arr / (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-9)\n",
        "        feats_all.append(arr.astype(np.float32))\n",
        "\n",
        "        # 釋放顯示卡暫存\n",
        "        if dev == 'cuda':\n",
        "            del feats, inputs\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    if len(feats_all) == 0:\n",
        "        return np.zeros((0, PROJ_DIM), dtype=np.float32)\n",
        "    return np.vstack(feats_all)\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_text_clip_safe(texts):\n",
        "    \"\"\"\n",
        "    保險版：自動嘗試不同 batch_size；仍 OOM 就改跑 CPU。\n",
        "    \"\"\"\n",
        "    for bs in [128, 64, 32, 16, 8, 4, 2, 1]:\n",
        "        try:\n",
        "            return embed_text_clip(texts, batch_size=bs, max_length=64, device_override=None, use_fp16=True)\n",
        "        except RuntimeError as e:\n",
        "            if 'CUDA out of memory' in str(e):\n",
        "                torch.cuda.empty_cache()\n",
        "                continue\n",
        "            else:\n",
        "                raise\n",
        "    # 退而求其次：改用 CPU（會慢，但能跑完）\n",
        "    return embed_text_clip(texts, batch_size=64, max_length=64, device_override='cpu', use_fp16=False)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_images_clip(pil_images):\n",
        "    \"\"\"\n",
        "    pil_images: list[PIL.Image]\n",
        "    return: np.array [n, PROJ_DIM], L2-normalized\n",
        "    \"\"\"\n",
        "    if len(pil_images) == 0:\n",
        "        return np.zeros((0, PROJ_DIM), dtype=np.float32)\n",
        "    inputs = clip_processor(images=pil_images, return_tensors='pt').to(device)\n",
        "    feats = clip_model.get_image_features(**inputs)  # [n, d]\n",
        "    arr = feats.detach().cpu().numpy()\n",
        "    arr = arr / (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-9)\n",
        "    return arr.astype(np.float32)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HUUCzhXrPQ-I"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# ==== Cell 7：數值型 Metadata 特徵函式 =========\n",
        "# =========================================================\n",
        "PROMO_WORDS  = ['折', '折扣', '%off', '% off', '促銷', '滿', '送', '優惠', '特價', '買一送一', '買一送二', '限時', '早鳥']\n",
        "FLAVOR_WORDS = ['芒果', '草莓', '葡萄', '百香', '抹茶', '烏龍', '紅茶', '綠茶', '奶蓋', '珍珠', '椰果', '仙草']\n",
        "HEALTH_WORDS = ['無糖', '微糖', '半糖', '少冰', '去冰', '低卡', '健康', '無添加']\n",
        "CTA_WORDS    = ['快來', '立刻', '今天', '現在', '一起', '打卡', '留言', '分享', '抽獎']\n",
        "PRICE_PAT    = r'(nt\\$|n\\$|\\$|元)\\s*\\d+'\n",
        "PCT_PAT      = r'\\d+\\s*%'\n",
        "\n",
        "def build_numeric_features(df, text_col, ocr_col, time_col):\n",
        "    \"\"\"\n",
        "    df: 需要包含 text_col（caption）、ocr_col（OCR 文字）、time_col（ftime_parsed）\n",
        "    回傳: 僅數值欄位的 DataFrame（會在後續做 z-score）\n",
        "    \"\"\"\n",
        "    caps = df[text_col].fillna('').astype(str)\n",
        "    ocrs = df[ocr_col].fillna('').astype(str)\n",
        "\n",
        "    feat = {}\n",
        "    # Caption 統計\n",
        "    feat['cap_len']       = caps.apply(lambda s: len(s))\n",
        "    feat['cap_hashtags']  = caps.apply(lambda s: len(re.findall(r'#\\w+', s)))\n",
        "    feat['cap_mentions']  = caps.apply(lambda s: len(re.findall(r'@\\w+', s)))\n",
        "    feat['cap_digits']    = caps.apply(lambda s: len(re.findall(r'\\d', s)))\n",
        "    feat['cap_bang']      = caps.apply(lambda s: s.count('!'))\n",
        "    feat['cap_qmark']     = caps.apply(lambda s: s.count('?'))\n",
        "    feat['cap_emoji']     = caps.apply(count_emojis)\n",
        "    feat['cap_promo']     = caps.apply(lambda s: has_any(s, PROMO_WORDS))\n",
        "    feat['cap_flavor']    = caps.apply(lambda s: has_any(s, FLAVOR_WORDS))\n",
        "    feat['cap_health']    = caps.apply(lambda s: has_any(s, HEALTH_WORDS))\n",
        "    feat['cap_cta']       = caps.apply(lambda s: has_any(s, CTA_WORDS))\n",
        "\n",
        "    # OCR 統計\n",
        "    feat['ocr_len']       = ocrs.apply(lambda s: len(s))\n",
        "    feat['ocr_digits']    = ocrs.apply(lambda s: len(re.findall(r'\\d', s)))\n",
        "    feat['ocr_has_price'] = ocrs.apply(lambda s: int(re.search(PRICE_PAT, s.lower()) is not None))\n",
        "    feat['ocr_has_pct']   = ocrs.apply(lambda s: int(re.search(PCT_PAT,   s.lower()) is not None))\n",
        "    feat['ocr_promo']     = ocrs.apply(lambda s: has_any(s, PROMO_WORDS))\n",
        "    feat['ocr_flavor']    = ocrs.apply(lambda s: has_any(s, FLAVOR_WORDS))\n",
        "    feat['ocr_health']    = ocrs.apply(lambda s: has_any(s, HEALTH_WORDS))\n",
        "    feat['ocr_cta']       = ocrs.apply(lambda s: has_any(s, CTA_WORDS))\n",
        "\n",
        "    # 時間特徵（使用 ftime_parsed）\n",
        "    t = pd.to_datetime(df[time_col], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "    hours = t.dt.hour.fillna(0).astype(int)\n",
        "    feat['time_sin'] = np.sin(2*np.pi*hours/24)\n",
        "    feat['time_cos'] = np.cos(2*np.pi*hours/24)\n",
        "\n",
        "    numeric_df = pd.DataFrame(feat, index=df.index)\n",
        "    return numeric_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXhYbXFVPSbY",
        "outputId": "7eae02d1-6744-49a5-ea6f-658b93b47f25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "OCR train: 100%|██████████| 1788/1788 [00:04<00:00, 382.17it/s]\n",
            "/tmp/ipython-input-1801108807.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  ctx = torch.cuda.amp.autocast(enabled=use_amp, dtype=torch.float16) if use_amp else torch.autocast('cpu', enabled=False)\n",
            "Image emb train: 100%|██████████| 1788/1788 [02:54<00:00, 10.26it/s]\n",
            "OCR test: 100%|██████████| 763/763 [18:35<00:00,  1.46s/it]\n",
            "/tmp/ipython-input-1801108807.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  ctx = torch.cuda.amp.autocast(enabled=use_amp, dtype=torch.float16) if use_amp else torch.autocast('cpu', enabled=False)\n",
            "Image emb test: 100%|██████████| 763/763 [01:11<00:00, 10.66it/s]\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# ==== Cell 8 產生三模態嵌入與 OCR ==========\n",
        "# ======================================================\n",
        "\n",
        "# === 影像嵌入快取工具 ===\n",
        "import hashlib, os\n",
        "IMG_EMB_DIR = os.path.join(CACHE_DIR, 'img_emb_cache')\n",
        "os.makedirs(IMG_EMB_DIR, exist_ok=True)\n",
        "\n",
        "def _img_key(path, model_tag=\"\"):\n",
        "    # 把模型身分也納入key，避免換模型卻沿用舊向量\n",
        "    base = f\"{model_tag}|{path}\"\n",
        "    return hashlib.sha1(base.encode('utf-8', 'ignore')).hexdigest()\n",
        "\n",
        "def get_img_emb_with_cache(pil_img, path_hint, model_tag=\"\"):\n",
        "    key = _img_key(path_hint, model_tag)\n",
        "    cache_path = os.path.join(IMG_EMB_DIR, f'{key}.npy')\n",
        "    if os.path.exists(cache_path):\n",
        "        return np.load(cache_path)\n",
        "    arr = embed_images_clip([pil_img])  # [1, PROJ_DIM]\n",
        "    vec = arr[0]\n",
        "    np.save(cache_path, vec)\n",
        "    return vec\n",
        "\n",
        "# 在 Cell 6 設定的模型身分（例如 'chinese-clip' 或 'openai-clip'）\n",
        "MODEL_TAG = MODEL_BACKEND  # 也可用 MODEL_ID_CN/MODEL_ID_EN 更細\n",
        "\n",
        "def build_modal_embeddings(df, img_dir, split_name='train'):\n",
        "    \"\"\"\n",
        "    回傳：\n",
        "      cap_emb: CLIP text embedding (caption)\n",
        "      ocr_emb: CLIP text embedding (OCR 文字)\n",
        "      img_emb: CLIP image embedding（多圖平均）\n",
        "      ocr_texts: OCR 後的文字\n",
        "      rel_lists : 每列的相對路徑清單\n",
        "    \"\"\"\n",
        "    rel_lists = df['rel_img_paths'].apply(parse_rel_img_paths).tolist()\n",
        "\n",
        "    # 1) OCR to text（有快取）\n",
        "    ocr_texts = []\n",
        "    for i, rels in tqdm(list(enumerate(rel_lists)), desc=f'OCR {split_name}', total=len(rel_lists)):\n",
        "        brand = str(df.iloc[i][brand_col])\n",
        "        key = f\"{split_name}_{brand}_{i}\"\n",
        "        ocr_texts.append(ocr_post(rels, img_dir, key))\n",
        "\n",
        "    # 2) Caption / OCR embeddings\n",
        "    cap_texts = df[caption_col].fillna('').astype(str).tolist()\n",
        "    #cap_emb   = embed_text_clip(cap_texts)\n",
        "    #ocr_emb   = embed_text_clip(ocr_texts)\n",
        "\n",
        "    # 保險版：\n",
        "    cap_emb = embed_text_clip_safe(cap_texts)\n",
        "    ocr_emb = embed_text_clip_safe(ocr_texts)\n",
        "\n",
        "    # 3) Image embeddings（逐張快取，最後平均）\n",
        "    img_embs = []\n",
        "    for rels in tqdm(rel_lists, desc=f'Image emb {split_name}', total=len(rel_lists)):\n",
        "        vecs = []\n",
        "        for rp in rels[:IMG_MAX_IMAGES]:\n",
        "            p = os.path.join(img_dir, rp)\n",
        "            if not os.path.exists(p):\n",
        "                p2 = os.path.join(img_dir, os.path.basename(rp))\n",
        "                if os.path.exists(p2):\n",
        "                    p = p2\n",
        "                else:\n",
        "                    continue\n",
        "            im = load_image_for_clip(p)\n",
        "            if im is None:\n",
        "                continue\n",
        "            v = get_img_emb_with_cache(im, p, MODEL_TAG)\n",
        "            vecs.append(v)\n",
        "        if len(vecs) == 0:\n",
        "            mean_vec = np.zeros((PROJ_DIM,), dtype=np.float32)\n",
        "        else:\n",
        "            arr = np.vstack(vecs)\n",
        "            mean_vec = arr.mean(axis=0)\n",
        "            mean_vec = mean_vec / (np.linalg.norm(mean_vec) + 1e-9)  # L2 normalize\n",
        "        img_embs.append(mean_vec.astype(np.float32))\n",
        "    img_emb = np.vstack(img_embs)\n",
        "\n",
        "    return cap_emb, ocr_emb, img_emb, ocr_texts, rel_lists\n",
        "\n",
        "# 產生三模態嵌入\n",
        "cap_train, ocr_train, img_train, ocr_text_train, rels_train = build_modal_embeddings(train, IMG_TRAIN_DIR, 'train')\n",
        "cap_test,  ocr_test,  img_test,  ocr_text_test,  rels_test  = build_modal_embeddings(test,  IMG_TEST_DIR,  'test')\n",
        "\n",
        "# 存放 OCR 文字，之後輸出 CSV 用\n",
        "train['ocr_text'] = ocr_text_train\n",
        "test['ocr_text']  = ocr_text_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TtieuxK8PT7Y"
      },
      "outputs": [],
      "source": [
        "# =================================================\n",
        "# ==== Cell 9: 數值型 Metadata（z-score + 縮放） ====\n",
        "# =================================================\n",
        "numeric_train = build_numeric_features(train, caption_col, 'ocr_text', time_col=time_col)\n",
        "numeric_test  = build_numeric_features(test,  caption_col, 'ocr_text', time_col=time_col)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "numeric_train_z = pd.DataFrame(scaler.fit_transform(numeric_train), columns=numeric_train.columns, index=train.index)\n",
        "numeric_test_z  = pd.DataFrame(scaler.transform(numeric_test),  columns=numeric_test.columns,  index=test.index)\n",
        "\n",
        "# 將數值型向量轉為 numpy\n",
        "num_train_vec = numeric_train_z.values.astype(np.float32)\n",
        "num_test_vec  = numeric_test_z.values.astype(np.float32)\n",
        "\n",
        "# --- 縮放 numeric 區塊，避免壓過語意 ---\n",
        "# 先計算語意區塊（cap+ocr+img）的平均 L2 範數，再把 numeric 區塊縮到語意的一定比例（例如 0.5）\n",
        "sem_train_vec = np.hstack([cap_train, ocr_train, img_train])\n",
        "sem_norm_mean = np.mean(np.linalg.norm(sem_train_vec, axis=1))\n",
        "num_norm_mean = np.mean(np.linalg.norm(num_train_vec, axis=1) + 1e-9)\n",
        "delta = 0.5 * (sem_norm_mean / (num_norm_mean + 1e-9))  # 讓 numeric 平均範數約為語意的一半\n",
        "num_train_vec_scaled = num_train_vec * delta\n",
        "num_test_vec_scaled  = num_test_vec  * delta\n",
        "\n",
        "# ---- 構成 joint_emb（全模態用於 Phase 1 子模態切分）----\n",
        "# Text 模態 = cap_emb + ocr_emb\n",
        "text_train_vec = np.hstack([cap_train, ocr_train]).astype(np.float32)\n",
        "text_test_vec  = np.hstack([cap_test,  ocr_test ]).astype(np.float32)\n",
        "# Image 模態\n",
        "image_train_vec = img_train.astype(np.float32)\n",
        "image_test_vec  = img_test.astype(np.float32)\n",
        "# Meta 模態（純 numeric）\n",
        "meta_train_vec  = num_train_vec_scaled.astype(np.float32)\n",
        "meta_test_vec   = num_test_vec_scaled.astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eK7aJuEHPVkw"
      },
      "outputs": [],
      "source": [
        "# =======================================\n",
        "# ==== Cell 10: Novelty / Diversity  ====\n",
        "# =======================================\n",
        "def fit_anchors(X, k=K_CLUSTERS, random_state=SEED):\n",
        "    \"\"\"\n",
        "    訓練期建立 K-means 中心（錨點）\n",
        "    這裡使用 MiniBatchKMeans 以支援 sample_weight（較節省記憶體）\n",
        "    \"\"\"\n",
        "    km = MiniBatchKMeans(n_clusters=k, random_state=random_state, batch_size=512, n_init='auto')\n",
        "    km.fit(X)\n",
        "    centers = km.cluster_centers_\n",
        "    # 對中心做 L2 normalize（讓 cosine 更穩）\n",
        "    centers = centers / (np.linalg.norm(centers, axis=1, keepdims=True) + 1e-9)\n",
        "    return centers\n",
        "\n",
        "def compute_novelty(X, centers):\n",
        "    \"\"\"\n",
        "    Novelty = 1 - max cosine similarity to centers\n",
        "    \"\"\"\n",
        "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
        "    sims = np.dot(Xn, centers.T)  # [n, k]\n",
        "    max_sim = sims.max(axis=1)\n",
        "    nov_raw = 1.0 - max_sim\n",
        "    return nov_raw, sims  # sims 備用給 diversity\n",
        "\n",
        "def compute_diversity_from_sims(sims, tau=TAU):\n",
        "    \"\"\"\n",
        "    Diversity = normalized entropy of softmax(sim/tau)\n",
        "    \"\"\"\n",
        "    probs = np.vstack([softmax(row, tau) for row in sims])  # [n, k]\n",
        "    # entropy\n",
        "    ent = -(probs * (np.log(probs + 1e-9))).sum(axis=1)\n",
        "    # normalize by log(k)\n",
        "    k = sims.shape[1]\n",
        "    ent_norm = ent / (math.log(k) + 1e-9)\n",
        "    return ent_norm\n",
        "\n",
        "def minmax_fit(x):\n",
        "    mn, mx = float(np.min(x)), float(np.max(x))\n",
        "    if mx - mn < 1e-9:\n",
        "        return mn, mx\n",
        "    return mn, mx\n",
        "\n",
        "def minmax_transform(x, mn, mx):\n",
        "    if mx - mn < 1e-9:\n",
        "        return np.zeros_like(x, dtype=np.float32)\n",
        "    return ((x - mn) / (mx - mn)).astype(np.float32)\n",
        "\n",
        "def compute_modality_scores(train_vec, test_vec, k=K_CLUSTERS, tau=TAU):\n",
        "    # anchors\n",
        "    centers = fit_anchors(train_vec, k=k)\n",
        "    # novelty\n",
        "    nov_tr_raw, sims_tr = compute_novelty(train_vec, centers)\n",
        "    nov_te_raw, sims_te = compute_novelty(test_vec,  centers)\n",
        "    # min-max（訓練集為準，離群納入）\n",
        "    mn, mx = minmax_fit(nov_tr_raw)\n",
        "    nov_tr = minmax_transform(nov_tr_raw, mn, mx)\n",
        "    nov_te = minmax_transform(nov_te_raw, mn, mx)\n",
        "    # diversity\n",
        "    div_tr = compute_diversity_from_sims(sims_tr, tau=tau).astype(np.float32)\n",
        "    div_te = compute_diversity_from_sims(sims_te, tau=tau).astype(np.float32)\n",
        "    return {\n",
        "        'centers': centers,\n",
        "        'nov_tr': nov_tr, 'nov_te': nov_te,\n",
        "        'div_tr': div_tr, 'div_te': div_te,\n",
        "        'nov_min': mn, 'nov_max': mx\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Oy79jNyXPXDA"
      },
      "outputs": [],
      "source": [
        "# =========================================\n",
        "# ==== Cell 11: Phase 1（各模態學權重）====\n",
        "# =========================================\n",
        "# 目標變數 y（其實可以再改!!）\n",
        "def compute_y(likes, comments, followers):\n",
        "    return (likes + 5.0*comments) / (followers + 0.01)\n",
        "\n",
        "train['y'] = compute_y(train['count_like'].fillna(0).to_numpy(),\n",
        "                       train['count_comment'].fillna(0).to_numpy(),\n",
        "                       train['followers'].fillna(0).to_numpy())\n",
        "test['y']  = compute_y(test['count_like'].fillna(0).to_numpy(),\n",
        "                       test['count_comment'].fillna(0).to_numpy(),\n",
        "                       test['followers'].fillna(0).to_numpy())\n",
        "\n",
        "# brand 反比權重（用於回歸訓練）\n",
        "brand_counts = train[brand_col].value_counts().to_dict()\n",
        "train_weights = train[brand_col].map(lambda b: 1.0 / math.sqrt(brand_counts.get(b,1))).to_numpy()\n",
        "\n",
        "# late-entry 標記（訓練為 0 篇、測試 > 0 篇）\n",
        "train_brands = set(train[brand_col].unique().tolist())\n",
        "test_brands  = set(test[brand_col].unique().tolist())\n",
        "late_entry_brands = sorted(list(test_brands - train_brands))\n",
        "test['is_late_entry_brand'] = test[brand_col].isin(late_entry_brands).astype(int)\n",
        "\n",
        "def learn_wN_wD(nov_tr, div_tr, y_tr, sample_weight=None):\n",
        "    X = np.vstack([nov_tr, div_tr]).T.astype(np.float32)\n",
        "    lr = LinearRegression()\n",
        "    if sample_weight is not None:\n",
        "        lr.fit(X, y_tr, sample_weight=sample_weight)\n",
        "    else:\n",
        "        lr.fit(X, y_tr)\n",
        "    beta = lr.coef_.astype(np.float32)  # [beta_N, beta_D]\n",
        "    # 截斷負值，避免解釋衝突；全為 0 時設為均分\n",
        "    beta = np.maximum(beta, 0.0)\n",
        "    if beta.sum() < 1e-9:\n",
        "        wN, wD = 0.5, 0.5\n",
        "    else:\n",
        "        wN, wD = (beta / (beta.sum()+1e-9)).tolist()\n",
        "    return float(wN), float(wD), lr\n",
        "\n",
        "def phase1_per_modality(train_vec, test_vec, y_tr, sample_weight=None, name='text'):\n",
        "    pack = compute_modality_scores(train_vec, test_vec, k=K_CLUSTERS, tau=TAU)\n",
        "    wN, wD, lr = learn_wN_wD(pack['nov_tr'], pack['div_tr'], y_tr, sample_weight=sample_weight)\n",
        "    # Distinctiveness Score（DS）與 風險 ATI\n",
        "    DS_tr = (wN*pack['nov_tr'] + wD*pack['div_tr']).astype(np.float32)\n",
        "    DS_te = (wN*pack['nov_te'] + wD*pack['div_te']).astype(np.float32)\n",
        "    ATI_tr = 100.0*(1.0 - DS_tr)\n",
        "    ATI_te = 100.0*(1.0 - DS_te)\n",
        "    return {\n",
        "        'name': name,\n",
        "        'centers': pack['centers'],\n",
        "        'nov_tr': pack['nov_tr'], 'nov_te': pack['nov_te'],\n",
        "        'div_tr': pack['div_tr'], 'div_te': pack['div_te'],\n",
        "        'wN': wN, 'wD': wD,\n",
        "        'DS_tr': DS_tr, 'DS_te': DS_te,\n",
        "        'ATI_tr': ATI_tr, 'ATI_te': ATI_te,\n",
        "        'nov_min': pack['nov_min'], 'nov_max': pack['nov_max']\n",
        "    }\n",
        "\n",
        "# 三個模態跑 Phase 1\n",
        "phase1_text  = phase1_per_modality(text_train_vec,  text_test_vec,  train['y'].to_numpy(), sample_weight=train_weights, name='text')\n",
        "phase1_image = phase1_per_modality(image_train_vec, image_test_vec, train['y'].to_numpy(), sample_weight=train_weights, name='image')\n",
        "phase1_meta  = phase1_per_modality(meta_train_vec,  meta_test_vec,  train['y'].to_numpy(), sample_weight=train_weights, name='meta')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLlPoKhnPYkI",
        "outputId": "c879235d-84ba-4de4-ef35-e14da183e845"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'phase1_text_wN': 0.2099631130695343,\n",
              " 'phase1_text_wD': 0.7900368571281433,\n",
              " 'phase1_image_wN': 1.0,\n",
              " 'phase1_image_wD': 0.0,\n",
              " 'phase1_meta_wN': 0.0,\n",
              " 'phase1_meta_wD': 1.0,\n",
              " 'phase2_v_text': 0.5774988532066345,\n",
              " 'phase2_v_image': 0.2962433099746704,\n",
              " 'phase2_v_meta': 0.12625786662101746}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =========================================\n",
        "# ==== Cell 12: Phase 2（二階段合成） ======\n",
        "# =========================================\n",
        "# 依老師建議：用三個「模態 ATI」來合成最終 ATI\n",
        "# 但為了與 y 呈現正向關係更直觀，在回歸時用「DS」（越大越好）來學權重，再轉回 ATI。\n",
        "\n",
        "# Train 組合\n",
        "DS_text_tr  = phase1_text['DS_tr']\n",
        "DS_image_tr = phase1_image['DS_tr']\n",
        "DS_meta_tr  = phase1_meta['DS_tr']\n",
        "X_tr = np.vstack([DS_text_tr, DS_image_tr, DS_meta_tr]).T.astype(np.float32)\n",
        "y_tr = train['y'].to_numpy()\n",
        "\n",
        "# 以 brand 反比權重做二階段回歸\n",
        "lr2 = LinearRegression()\n",
        "lr2.fit(X_tr, y_tr, sample_weight=train_weights)\n",
        "coef2 = np.maximum(lr2.coef_.astype(np.float32), 0.0)\n",
        "if coef2.sum() < 1e-9:\n",
        "    v = np.array([1/3, 1/3, 1/3], dtype=np.float32)\n",
        "else:\n",
        "    v = coef2 / coef2.sum()\n",
        "\n",
        "# Test 組合\n",
        "DS_text_te  = phase1_text['DS_te']\n",
        "DS_image_te = phase1_image['DS_te']\n",
        "DS_meta_te  = phase1_meta['DS_te']\n",
        "DS_final_tr = (v[0]*DS_text_tr + v[1]*DS_image_tr + v[2]*DS_meta_tr).astype(np.float32)\n",
        "DS_final_te = (v[0]*DS_text_te + v[1]*DS_image_te + v[2]*DS_meta_te).astype(np.float32)\n",
        "\n",
        "ATI_final_tr = 100.0*(1.0 - DS_final_tr)\n",
        "ATI_final_te = 100.0*(1.0 - DS_final_te)\n",
        "\n",
        "# 保存權重，方便簡報\n",
        "weights_summary = {\n",
        "    'phase1_text_wN': phase1_text['wN'], 'phase1_text_wD': phase1_text['wD'],\n",
        "    'phase1_image_wN': phase1_image['wN'], 'phase1_image_wD': phase1_image['wD'],\n",
        "    'phase1_meta_wN': phase1_meta['wN'], 'phase1_meta_wD': phase1_meta['wD'],\n",
        "    'phase2_v_text': float(v[0]), 'phase2_v_image': float(v[1]), 'phase2_v_meta': float(v[2]),\n",
        "}\n",
        "weights_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUfNr26mPbvg",
        "outputId": "990ba096-b446-4580-ab4b-53e4e01872de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/project/new/outputs/ati_train_per_post.csv',\n",
              " '/content/drive/MyDrive/project/new/outputs/ati_test_per_post.csv',\n",
              " '/content/drive/MyDrive/project/new/outputs/ati_test_brand_agg.csv')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =========================================\n",
        "# ==== Cell 13: 輸出 per-post & per-brand ===\n",
        "# =========================================\n",
        "# Per-post（train/test）\n",
        "post_train_out = train[[brand_col, 'count_like','count_comment','followers']].copy()\n",
        "post_test_out  = test[[brand_col,  'count_like','count_comment','followers','is_late_entry_brand']].copy()\n",
        "\n",
        "post_train_out['y'] = train['y'].values\n",
        "post_test_out['y']  = test['y'].values\n",
        "\n",
        "# 各模態 N/D 與 ATI\n",
        "def add_phase1_outputs(df_out, phase1, split='tr'):\n",
        "    df_out[f'{phase1[\"name\"]}_nov'] = phase1[f'nov_{split}']\n",
        "    df_out[f'{phase1[\"name\"]}_div'] = phase1[f'div_{split}']\n",
        "    df_out[f'{phase1[\"name\"]}_DS']  = phase1[f'DS_{split}']\n",
        "    df_out[f'{phase1[\"name\"]}_ATI'] = phase1[f'ATI_{split}']\n",
        "\n",
        "add_phase1_outputs(post_train_out, phase1_text,  'tr')\n",
        "add_phase1_outputs(post_train_out, phase1_image, 'tr')\n",
        "add_phase1_outputs(post_train_out, phase1_meta,  'tr')\n",
        "post_train_out['ATI_final'] = ATI_final_tr\n",
        "post_train_out['DS_final']  = DS_final_tr\n",
        "\n",
        "add_phase1_outputs(post_test_out, phase1_text,  'te')\n",
        "add_phase1_outputs(post_test_out, phase1_image, 'te')\n",
        "add_phase1_outputs(post_test_out, phase1_meta,  'te')\n",
        "post_test_out['ATI_final'] = ATI_final_te\n",
        "post_test_out['DS_final']  = DS_final_te\n",
        "\n",
        "# 加入文字欄位（方便檢視）\n",
        "post_train_out['caption']  = train[caption_col]\n",
        "post_train_out['ocr_text'] = train['ocr_text']\n",
        "post_test_out['caption']   = test[caption_col]\n",
        "post_test_out['ocr_text']  = test['ocr_text']\n",
        "\n",
        "# 存檔\n",
        "train_csv_out = os.path.join(OUT_DIR, 'ati_train_per_post.csv')\n",
        "test_csv_out  = os.path.join(OUT_DIR, 'ati_test_per_post.csv')\n",
        "post_train_out.to_csv(train_csv_out, index=False)\n",
        "post_test_out.to_csv(test_csv_out, index=False)\n",
        "\n",
        "# 品牌彙總（測試期）\n",
        "brand_test_agg = post_test_out.groupby(brand_col).agg(\n",
        "    n_posts=('ATI_final','size'),\n",
        "    ATI_final_mean=('ATI_final','mean'),\n",
        "    DS_final_mean=('DS_final','mean'),\n",
        "    y_mean=('y','mean'),\n",
        "    late_entry_brand=('is_late_entry_brand','max')\n",
        ").reset_index()\n",
        "\n",
        "brand_csv_out = os.path.join(OUT_DIR, 'ati_test_brand_agg.csv')\n",
        "brand_test_agg.to_csv(brand_csv_out, index=False)\n",
        "\n",
        "train_csv_out, test_csv_out, brand_csv_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC1cxd_ePcGQ",
        "outputId": "b2e11807-f19a-4dfd-80a2-a858b5d8dd90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phase 1: Text  vs y (train/test) = (0.06916880756981825, 0.003430802333130438) (0.02678914278252213, 0.4599669400694495)\n",
            "Phase 1: Image vs y (train/test) = (-0.02068385985558081, 0.38206723592976377) (-0.015096507131131172, 0.6771593017667055)\n",
            "Phase 1: Meta  vs y (train/test) = (0.012035383538588231, 0.6110496706422119) (0.042964186154434696, 0.23586805017425477)\n",
            "Phase 2: DS_final vs y (train/test) = (0.04079031588831303, 0.08465014596754702) (0.008771688936276589, 0.8088553345224511)\n",
            "輸出檔： /content/drive/MyDrive/project/new/outputs/ati_train_per_post.csv /content/drive/MyDrive/project/new/outputs/ati_test_per_post.csv /content/drive/MyDrive/project/new/outputs/ati_test_brand_agg.csv\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# ==== Cell 14: 簡單檢查 ==========\n",
        "# =========================================\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def spearman_safe(a, b):\n",
        "    try:\n",
        "        r, p = spearmanr(a, b)\n",
        "        return float(r), float(p)\n",
        "    except Exception:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "print(\"Phase 1: Text  vs y (train/test) =\",\n",
        "      spearman_safe(phase1_text['DS_tr'], train['y']),\n",
        "      spearman_safe(phase1_text['DS_te'], test['y']))\n",
        "print(\"Phase 1: Image vs y (train/test) =\",\n",
        "      spearman_safe(phase1_image['DS_tr'], train['y']),\n",
        "      spearman_safe(phase1_image['DS_te'], test['y']))\n",
        "print(\"Phase 1: Meta  vs y (train/test) =\",\n",
        "      spearman_safe(phase1_meta['DS_tr'], train['y']),\n",
        "      spearman_safe(phase1_meta['DS_te'], test['y']))\n",
        "\n",
        "print(\"Phase 2: DS_final vs y (train/test) =\",\n",
        "      spearman_safe(( (phase1_text['DS_tr']*weights_summary['phase2_v_text'] +\n",
        "                       phase1_image['DS_tr']*weights_summary['phase2_v_image'] +\n",
        "                       phase1_meta['DS_tr']*weights_summary['phase2_v_meta']) ), train['y']),\n",
        "      spearman_safe(( (phase1_text['DS_te']*weights_summary['phase2_v_text'] +\n",
        "                       phase1_image['DS_te']*weights_summary['phase2_v_image'] +\n",
        "                       phase1_meta['DS_te']*weights_summary['phase2_v_meta']) ), test['y'])\n",
        "     )\n",
        "\n",
        "print(\"輸出檔：\", train_csv_out, test_csv_out, brand_csv_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH1iRPoZ5Dfw",
        "outputId": "ceeea5c2-3fb1-4992-9005-1ea20c17dcbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DS_final vs y: Spearman rho=0.009, p=8.09e-01\n",
            "ATI_final vs y: Spearman rho=-0.009, p=8.09e-01\n"
          ]
        }
      ],
      "source": [
        "#(a) Spearman 相關（方向 sanity check）\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "import os\n",
        "\n",
        "df_te = pd.read_csv(os.path.join(BASE_DIR, 'outputs', 'ati_test_per_post.csv'))\n",
        "\n",
        "pairs = [\n",
        "    ('DS_text','y'), ('DS_image','y'), ('DS_meta','y'), ('DS_final','y'),\n",
        "    ('ATI_text','y'), ('ATI_image','y'), ('ATI_meta','y'), ('ATI_final','y'),\n",
        "]\n",
        "for a,b in pairs:\n",
        "    if a in df_te.columns and b in df_te.columns:\n",
        "        r,p = spearmanr(df_te[a], df_te[b], nan_policy='omit')\n",
        "        print(f\"{a} vs {b}: Spearman rho={r:.3f}, p={p:.2e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEzK22Op5LiD",
        "outputId": "6daebb40-89b9-4986-c0ff-5727ec679619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   decile  count      mean    median\n",
            "0       0     77  0.076149  0.005997\n",
            "1       1     76  0.043754  0.006086\n",
            "2       2     76  0.531745  0.007406\n",
            "3       3     76  0.042767  0.010277\n",
            "4       4     77  0.053681  0.005075\n",
            "5       5     76  0.028108  0.006360\n",
            "6       6     76  0.099293  0.007413\n",
            "7       7     76  0.032188  0.006286\n",
            "8       8     76  0.034443  0.009213\n",
            "9       9     77  0.126797  0.004550\n"
          ]
        }
      ],
      "source": [
        "#(b) 分箱（deciles）看單調性\n",
        "import numpy as np\n",
        "\n",
        "# 以 ATI_final 為主，把 test 貼文分成 10 等分\n",
        "df_te = df_te.dropna(subset=['ATI_final', 'y']).copy()\n",
        "df_te['decile'] = pd.qcut(df_te['ATI_final'], 10, labels=False, duplicates='drop')\n",
        "dec = df_te.groupby('decile')['y'].agg(['count','mean','median']).reset_index()\n",
        "print(dec)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
