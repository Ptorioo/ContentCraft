"""
ç”Ÿæˆä¿ç•™ ATI åˆ†å¸ƒçš„é™ç¶­åœ°åœ–

æ”¯æ´å¤šç¨®ç­–ç•¥ï¼š
1. ATI_as_axis: å°‡ ATI ä½œç‚º Y è»¸ï¼Œembedding é™ç¶­ä½œç‚º X è»¸
2. ATI_as_radius: å°‡ ATI æ˜ å°„åˆ°æ¥µåº§æ¨™çš„åŠå¾‘ï¼ˆè·é›¢ä¸­å¿ƒï¼‰ï¼Œè§’åº¦ç”± embedding æ±ºå®š
3. ATI_in_corner: ä½¿ç”¨ç›£ç£å¼é™ç¶­ï¼Œè®“é«˜ ATI å“ç‰Œä½æ–¼å³ä¸Šè§’
"""

import numpy as np
import pandas as pd
import json
from pathlib import Path
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
import warnings
warnings.filterwarnings('ignore')

# è¨­å®šè·¯å¾‘
ROOT = Path(__file__).parent.parent
EMBEDDING_FILE = ROOT / 'çµæœ' / 'modal_embeddings_v2.npz'
TEST_CSV = ROOT / 'çµæœ' / 'ati_test_per_post.csv'
TRAIN_CSV = ROOT / 'çµæœ' / 'ati_train_per_post.csv'
OUTPUT_FILE = ROOT / 'src' / 'data' / 'generated' / 'ati_aware_map.json'

# é™ç¶­ç­–ç•¥ï¼š
# - 'axis' (ATI ä½œç‚ºè»¸)
# - 'radius' (ATI ä½œç‚ºåŠå¾‘ï¼Œé«˜ ATI é é›¢ä¸­å¿ƒ)
# - 'corner' (é«˜ ATI åœ¨è§’è½)
# - 'distance' (é«˜ ATI é é›¢ä¸­å¿ƒ)
# - 'center' (é«˜ ATI æ¥è¿‘ä¸­å¿ƒ) <- æ–°ç­–ç•¥
# - 'cluster_center' (é«˜ ATI æ¥è¿‘å„è‡ªçš„èšé¡ä¸­å¿ƒ) <- æ–°ç­–ç•¥
STRATEGY = 'cluster_center'  # å¯ä»¥æ”¹ç‚º 'axis', 'radius', 'corner', 'distance', 'center', æˆ– 'cluster_center'

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)

print(f"ğŸ“Š é–‹å§‹ç”¢ç”Ÿä¿ç•™ ATI åˆ†å¸ƒçš„é™ç¶­åœ°åœ–ï¼ˆç­–ç•¥: {STRATEGY}ï¼‰...")

# 1. è®€å– embedding
print("\n1ï¸âƒ£ è®€å– embedding æª”æ¡ˆ...")
data = np.load(EMBEDDING_FILE, allow_pickle=True)
cap_test = data['cap_test']
ocr_test = data['ocr_test']
img_test = data['img_test']

cap_train = data.get('cap_train', None)
ocr_train = data.get('ocr_train', None)
img_train = data.get('img_train', None)

# 2. è®€å–å“ç‰Œè³‡æ–™
print("\n2ï¸âƒ£ è®€å–å“ç‰Œè³‡æ–™...")
df_test = pd.read_csv(TEST_CSV)
df_train = None
if TRAIN_CSV.exists():
    df_train = pd.read_csv(TRAIN_CSV)
    df_all = pd.concat([df_test, df_train], ignore_index=True)
else:
    df_all = df_test

# 3. è¨ˆç®—å“ç‰Œç´š embedding å’Œ ATI
print("\n3ï¸âƒ£ è¨ˆç®—å“ç‰Œç´š embedding...")
text_emb_test = np.hstack([cap_test, ocr_test])
full_emb_test = np.hstack([text_emb_test, img_test])

if cap_train is not None:
    text_emb_train = np.hstack([cap_train, ocr_train])
    full_emb_train = np.hstack([text_emb_train, img_train])
    full_emb = np.vstack([full_emb_test, full_emb_train])
else:
    full_emb = full_emb_test

brands = df_all['brand'].unique()
brand_embeddings = {}
brand_atis = {}
brand_info = {}

for brand in brands:
    brand_mask_test = df_test['brand'] == brand
    brand_indices_test = np.where(brand_mask_test)[0]
    
    brand_indices_train = []
    if df_train is not None:
        brand_mask_train = df_train['brand'] == brand
        brand_indices_train = np.where(brand_mask_train)[0] + len(df_test)
    
    all_indices = list(brand_indices_test) + list(brand_indices_train)
    
    if len(all_indices) == 0:
        continue
    
    brand_emb = full_emb[all_indices].mean(axis=0)
    brand_embeddings[brand] = brand_emb
    
    brand_posts = df_all[df_all['brand'] == brand]
    brand_atis[brand] = brand_posts['ATI_final'].mean()
    brand_info[brand] = {
        'n_posts': len(brand_posts),
        'ATI_final_mean': brand_posts['ATI_final'].mean(),
        'DS_final_mean': brand_posts['DS_final'].mean(),
        'y_mean': brand_posts['y'].mean(),
    }

print(f"   âœ“ è¨ˆç®—å®Œæˆï¼Œå…± {len(brand_embeddings)} å€‹å“ç‰Œ")

# 4. æº–å‚™é™ç¶­è³‡æ–™
print("\n4ï¸âƒ£ æº–å‚™é™ç¶­è³‡æ–™...")
brand_list = list(brand_embeddings.keys())
embeddings_matrix = np.array([brand_embeddings[brand] for brand in brand_list])
atis_array = np.array([brand_atis[brand] for brand in brand_list])

# æ¨™æº–åŒ– embedding
scaler = StandardScaler()
embeddings_scaled = scaler.fit_transform(embeddings_matrix)

# æ¨™æº–åŒ– ATI
ati_scaled = (atis_array - atis_array.mean()) / (atis_array.std() + 1e-9)
ati_normalized = (atis_array - atis_array.min()) / (atis_array.max() - atis_array.min() + 1e-9)  # 0-1 ç¯„åœ

print(f"   âœ“ æ¨™æº–åŒ–å®Œæˆ")
print(f"   âœ“ ATI ç¯„åœ: {atis_array.min():.2f} - {atis_array.max():.2f}")

# 5. æ ¹æ“šç­–ç•¥åŸ·è¡Œé™ç¶­
print(f"\n5ï¸âƒ£ åŸ·è¡Œ ATI-aware é™ç¶­ï¼ˆç­–ç•¥: {STRATEGY}ï¼‰...")

if STRATEGY == 'axis':
    # ç­–ç•¥1ï¼šATI ä½œç‚º Y è»¸
    print("   ğŸ¯ æ–¹æ³•ï¼šå°‡ ATI ä½œç‚º Y è»¸ï¼Œå° embedding é™ç¶­ä½œç‚º X è»¸")
    
    if UMAP_AVAILABLE:
        reducer_1d = umap.UMAP(n_components=1, n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
        x_coords = reducer_1d.fit_transform(embeddings_scaled).flatten()
        reduction_method = "UMAP_1D_with_ATI"
    else:
        pca_1d = PCA(n_components=1)
        x_coords = pca_1d.fit_transform(embeddings_scaled).flatten()
        reduction_method = "PCA_1D_with_ATI"
    
    y_coords = ati_scaled
    coordinates_2d = np.column_stack([x_coords, y_coords])
    
elif STRATEGY == 'radius':
    # ç­–ç•¥2ï¼šATI ä½œç‚ºæ¥µåº§æ¨™çš„åŠå¾‘ï¼ˆè·é›¢ä¸­å¿ƒï¼‰
    print("   ğŸ¯ æ–¹æ³•ï¼šå°‡ ATI æ˜ å°„åˆ°æ¥µåº§æ¨™çš„åŠå¾‘ï¼Œè§’åº¦ç”± embedding æ±ºå®š")
    
    # å° embedding é™ç¶­åˆ° 1D ä½œç‚ºè§’åº¦
    if UMAP_AVAILABLE:
        reducer_1d = umap.UMAP(n_components=1, n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
        angles_1d = reducer_1d.fit_transform(embeddings_scaled).flatten()
        reduction_method = "UMAP_polar_with_ATI"
    else:
        pca_1d = PCA(n_components=1)
        angles_1d = pca_1d.fit_transform(embeddings_scaled).flatten()
        reduction_method = "PCA_polar_with_ATI"
    
    # å°‡è§’åº¦æ¨™æº–åŒ–åˆ° [0, 2Ï€]
    angles_normalized = (angles_1d - angles_1d.min()) / (angles_1d.max() - angles_1d.min() + 1e-9) * 2 * np.pi
    
    # å°‡ ATI æ˜ å°„åˆ°åŠå¾‘ï¼ˆé«˜ ATI = é é›¢ä¸­å¿ƒï¼‰
    # ä½¿ç”¨å¹³æ–¹æ ¹è®“åˆ†å¸ƒæ›´å‡å‹»
    radius = np.sqrt(ati_normalized) * 2  # åŠå¾‘ç¯„åœ 0-2
    
    # è½‰æ›ç‚ºç›´è§’åº§æ¨™
    x_coords = radius * np.cos(angles_normalized)
    y_coords = radius * np.sin(angles_normalized)
    coordinates_2d = np.column_stack([x_coords, y_coords])
    
elif STRATEGY == 'corner':
    # ç­–ç•¥3ï¼šç›£ç£å¼é™ç¶­ï¼Œè®“é«˜ ATI åœ¨å³ä¸Šè§’
    print("   ğŸ¯ æ–¹æ³•ï¼šä½¿ç”¨ç›£ç£å¼é™ç¶­ï¼Œè®“é«˜ ATI å“ç‰Œä½æ–¼å³ä¸Šè§’")
    
    # å…ˆå° embedding é™ç¶­åˆ° 2D
    if UMAP_AVAILABLE:
        reducer_2d = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
        coords_2d = reducer_2d.fit_transform(embeddings_scaled)
        reduction_method = "UMAP_supervised_with_ATI"
    else:
        pca_2d = PCA(n_components=2)
        coords_2d = pca_2d.fit_transform(embeddings_scaled)
        reduction_method = "PCA_supervised_with_ATI"
    
    # æ¨™æº–åŒ–åº§æ¨™åˆ° [-1, 1] ç¯„åœ
    coords_2d[:, 0] = (coords_2d[:, 0] - coords_2d[:, 0].min()) / (coords_2d[:, 0].max() - coords_2d[:, 0].min() + 1e-9) * 2 - 1
    coords_2d[:, 1] = (coords_2d[:, 1] - coords_2d[:, 1].min()) / (coords_2d[:, 1].max() - coords_2d[:, 1].min() + 1e-9) * 2 - 1
    
    # è¨ˆç®— ATI èˆ‡å…©å€‹ç¶­åº¦çš„ç›¸é—œæ€§
    corr_x = np.corrcoef(atis_array, coords_2d[:, 0])[0, 1]
    corr_y = np.corrcoef(atis_array, coords_2d[:, 1])[0, 1]
    
    # èª¿æ•´æ–¹å‘ï¼Œè®“é«˜ ATI åœ¨å³ä¸Šè§’
    if corr_x < 0:
        coords_2d[:, 0] = -coords_2d[:, 0]
    if corr_y < 0:
        coords_2d[:, 1] = -coords_2d[:, 1]
    
    # å¼·åˆ¶èª¿æ•´ï¼šå°‡ ATI ç›´æ¥æ˜ å°„åˆ°å³ä¸Šè§’æ–¹å‘
    # è¨ˆç®—æ¯å€‹å“ç‰Œåˆ°å³ä¸Šè§’ (1, 1) çš„è·é›¢ï¼Œé«˜ ATI æ‡‰è©²æ›´æ¥è¿‘å³ä¸Šè§’
    # ä½†æˆ‘å€‘è¦è®“é«˜ ATI åœ¨å³ä¸Šè§’ï¼Œæ‰€ä»¥æ‡‰è©²è®“é«˜ ATI çš„åº§æ¨™æ›´æ¥è¿‘ (1, 1)
    
    # æ–¹æ³•ï¼šå°‡ ATI ä½œç‚ºæ¬Šé‡ï¼Œèª¿æ•´åº§æ¨™è®“é«˜ ATI å“ç‰Œæ›´æ¥è¿‘å³ä¸Šè§’
    # è¨ˆç®—ç•¶å‰åº§æ¨™åˆ°å³ä¸Šè§’çš„è·é›¢
    target_x, target_y = 1.0, 1.0
    
    # å°æ¯å€‹å“ç‰Œï¼Œæ ¹æ“š ATI èª¿æ•´åº§æ¨™
    for i in range(len(brand_list)):
        # ç•¶å‰åº§æ¨™
        curr_x, curr_y = coords_2d[i, 0], coords_2d[i, 1]
        
        # ATI æ¬Šé‡ï¼ˆ0-1ï¼Œé«˜ ATI æ¥è¿‘ 1ï¼‰
        ati_weight = ati_normalized[i]
        
        # å°‡åº§æ¨™å‘ç›®æ¨™æ–¹å‘ï¼ˆå³ä¸Šè§’ï¼‰ç§»å‹•ï¼Œç§»å‹•è·é›¢èˆ‡ ATI æˆæ­£æ¯”
        # ä½†ä¿ç•™åŸæœ‰çš„èªæ„çµæ§‹ï¼ˆä¸å®Œå…¨ç§»å‹•åˆ°å³ä¸Šè§’ï¼‰
        new_x = curr_x + (target_x - curr_x) * ati_weight * 0.6  # 60% çš„ç§»å‹•
        new_y = curr_y + (target_y - curr_y) * ati_weight * 0.6
        
        coords_2d[i, 0] = new_x
        coords_2d[i, 1] = new_y
    
    # é‡æ–°æ¨™æº–åŒ–åˆ°åˆç†ç¯„åœ
    coords_2d[:, 0] = (coords_2d[:, 0] - coords_2d[:, 0].mean()) / (coords_2d[:, 0].std() + 1e-9)
    coords_2d[:, 1] = (coords_2d[:, 1] - coords_2d[:, 1].mean()) / (coords_2d[:, 1].std() + 1e-9)
    
    x_coords = coords_2d[:, 0]
    y_coords = coords_2d[:, 1]
    coordinates_2d = coords_2d

elif STRATEGY == 'distance':
    # ç­–ç•¥4ï¼šé«˜ ATI å“ç‰Œé é›¢ä¸­å¿ƒ
    print("   ğŸ¯ æ–¹æ³•ï¼šä½¿ç”¨æ¥µåº§æ¨™ç³»çµ±ï¼ŒATI æ˜ å°„åˆ°åŠå¾‘ï¼ˆè·é›¢ä¸­å¿ƒï¼‰ï¼Œè§’åº¦ç”± embedding æ±ºå®š")
    
    # å° embedding é™ç¶­åˆ° 1D ä½œç‚ºè§’åº¦
    if UMAP_AVAILABLE:
        reducer_1d = umap.UMAP(n_components=1, n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
        angles_1d = reducer_1d.fit_transform(embeddings_scaled).flatten()
        reduction_method = "UMAP_polar_with_ATI_radius"
    else:
        pca_1d = PCA(n_components=1)
        angles_1d = pca_1d.fit_transform(embeddings_scaled).flatten()
        reduction_method = "PCA_polar_with_ATI_radius"
    
    # å°‡è§’åº¦æ¨™æº–åŒ–åˆ° [0, 2Ï€]
    angles_normalized = (angles_1d - angles_1d.min()) / (angles_1d.max() - angles_1d.min() + 1e-9) * 2 * np.pi
    
    # å°‡ ATI æ˜ å°„åˆ°åŠå¾‘ï¼ˆé«˜ ATI = é é›¢ä¸­å¿ƒï¼‰
    # ä½¿ç”¨å¹³æ–¹æ ¹è®“åˆ†å¸ƒæ›´å‡å‹»ï¼Œä¸¦è¨­ç½®æœ€å°åŠå¾‘é¿å…æ‰€æœ‰é»éƒ½åœ¨ä¸­å¿ƒ
    min_radius = 0.5  # æœ€å°åŠå¾‘
    max_radius = 3.0  # æœ€å¤§åŠå¾‘
    radius = min_radius + (max_radius - min_radius) * np.sqrt(ati_normalized)
    
    # è½‰æ›ç‚ºç›´è§’åº§æ¨™
    x_coords = radius * np.cos(angles_normalized)
    y_coords = radius * np.sin(angles_normalized)
    coordinates_2d = np.column_stack([x_coords, y_coords])

elif STRATEGY == 'center':
    # ç­–ç•¥5ï¼šé«˜ ATI å“ç‰Œæ¥è¿‘ä¸­å¿ƒ
    print("   ğŸ¯ æ–¹æ³•ï¼šä½¿ç”¨æ¥µåº§æ¨™ç³»çµ±ï¼ŒATI åå‘æ˜ å°„åˆ°åŠå¾‘ï¼ˆé«˜ ATI = æ¥è¿‘ä¸­å¿ƒï¼‰ï¼Œè§’åº¦ç”± embedding æ±ºå®š")
    
    # å° embedding é™ç¶­åˆ° 1D ä½œç‚ºè§’åº¦
    if UMAP_AVAILABLE:
        reducer_1d = umap.UMAP(n_components=1, n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
        angles_1d = reducer_1d.fit_transform(embeddings_scaled).flatten()
        reduction_method = "UMAP_polar_with_ATI_inverse_radius"
    else:
        pca_1d = PCA(n_components=1)
        angles_1d = pca_1d.fit_transform(embeddings_scaled).flatten()
        reduction_method = "PCA_polar_with_ATI_inverse_radius"
    
    # å°‡è§’åº¦æ¨™æº–åŒ–åˆ° [0, 2Ï€]
    angles_normalized = (angles_1d - angles_1d.min()) / (angles_1d.max() - angles_1d.min() + 1e-9) * 2 * np.pi
    
    # å°‡ ATI åå‘æ˜ å°„åˆ°åŠå¾‘ï¼ˆé«˜ ATI = æ¥è¿‘ä¸­å¿ƒï¼‰
    # ä½¿ç”¨ 1 - ati_normalized è®“é«˜ ATI å°æ‡‰å°åŠå¾‘
    min_radius = 0.3  # æœ€å°åŠå¾‘ï¼ˆé«˜ ATI å“ç‰Œï¼‰
    max_radius = 3.0  # æœ€å¤§åŠå¾‘ï¼ˆä½ ATI å“ç‰Œï¼‰
    radius = min_radius + (max_radius - min_radius) * (1 - np.sqrt(ati_normalized))
    
    # è½‰æ›ç‚ºç›´è§’åº§æ¨™
    x_coords = radius * np.cos(angles_normalized)
    y_coords = radius * np.sin(angles_normalized)
    coordinates_2d = np.column_stack([x_coords, y_coords])

elif STRATEGY == 'cluster_center':
    # ç­–ç•¥6ï¼šé«˜ ATI å“ç‰Œæ¥è¿‘å„è‡ªçš„èšé¡ä¸­å¿ƒ
    print("   ğŸ¯ æ–¹æ³•ï¼šå…ˆé€²è¡Œèšé¡ï¼Œç„¶å¾Œè®“é«˜ ATI å“ç‰Œæ¥è¿‘å„è‡ªçš„èšé¡ä¸­å¿ƒ")
    
    # å…ˆé€²è¡Œèšé¡
    n_clusters = 6
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters_pre = kmeans.fit_predict(embeddings_scaled)
    cluster_centers = kmeans.cluster_centers_
    
    # å° embedding é™ç¶­åˆ° 2Dï¼ˆä¿ç•™èªæ„çµæ§‹ï¼‰
    if UMAP_AVAILABLE:
        reducer_2d = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
        coords_2d = reducer_2d.fit_transform(embeddings_scaled)
        reduction_method = "UMAP_cluster_center_with_ATI"
    else:
        pca_2d = PCA(n_components=2)
        coords_2d = pca_2d.fit_transform(embeddings_scaled)
        reduction_method = "PCA_cluster_center_with_ATI"
    
    # è¨ˆç®—æ¯å€‹èšé¡çš„ä¸­å¿ƒï¼ˆåœ¨é™ç¶­å¾Œçš„ 2D ç©ºé–“ä¸­ï¼‰
    cluster_centers_2d = []
    for i in range(n_clusters):
        cluster_points = coords_2d[clusters_pre == i]
        if len(cluster_points) > 0:
            cluster_centers_2d.append(cluster_points.mean(axis=0))
        else:
            cluster_centers_2d.append(coords_2d.mean(axis=0))
    cluster_centers_2d = np.array(cluster_centers_2d)
    
    # èª¿æ•´æ¯å€‹å“ç‰Œçš„ä½ç½®ï¼šé«˜ ATI å“ç‰Œæ›´æ¥è¿‘å…¶èšé¡ä¸­å¿ƒ
    adjusted_coords = coords_2d.copy()
    for i in range(len(brand_list)):
        cluster_id = clusters_pre[i]
        cluster_center = cluster_centers_2d[cluster_id]
        current_pos = coords_2d[i]
        
        # è¨ˆç®—åˆ°èšé¡ä¸­å¿ƒçš„è·é›¢
        dist_to_center = np.linalg.norm(current_pos - cluster_center)
        
        # ATI æ¬Šé‡ï¼šé«˜ ATI (æ¥è¿‘ 1) æ™‚ï¼Œä½ç½®æ›´æ¥è¿‘èšé¡ä¸­å¿ƒ
        # ä½¿ç”¨ ati_normalizedï¼Œé«˜ ATI = æ¥è¿‘ 1ï¼Œæ‰€ä»¥ç§»å‹•æ›´å¤š
        ati_weight = ati_normalized[i]
        
        # è¨ˆç®—å¾ç•¶å‰ä½ç½®åˆ°èšé¡ä¸­å¿ƒçš„æ–¹å‘
        direction = cluster_center - current_pos
        if np.linalg.norm(direction) > 1e-9:
            direction = direction / np.linalg.norm(direction)
        
        # æ ¹æ“š ATI èª¿æ•´ä½ç½®ï¼šé«˜ ATI å“ç‰Œæ›´æ¥è¿‘ä¸­å¿ƒï¼ˆç§»å‹•æ›´å¤šï¼‰
        # ç§»å‹•è·é›¢ = åŸå§‹è·é›¢ * ati_weight * 0.95ï¼Œé«˜ ATI æ™‚ç§»å‹• 95% çš„è·é›¢
        # é€™æ¨£é«˜ ATI å“ç‰Œæœƒéå¸¸æ¥è¿‘èšé¡ä¸­å¿ƒ
        move_distance = dist_to_center * ati_weight * 0.95
        adjusted_coords[i] = current_pos + direction * move_distance
    
    x_coords = adjusted_coords[:, 0]
    y_coords = adjusted_coords[:, 1]
    coordinates_2d = adjusted_coords
    
    # æ›´æ–° clustersï¼ˆä½¿ç”¨é å…ˆè¨ˆç®—çš„ï¼‰
    clusters = clusters_pre

# è¨ˆç®—ç›¸é—œæ€§
ati_x_corr = np.corrcoef(atis_array, x_coords)[0, 1]
ati_y_corr = np.corrcoef(atis_array, y_coords)[0, 1]
center_x, center_y = np.mean(x_coords), np.mean(y_coords)
distances = np.sqrt((x_coords - center_x)**2 + (y_coords - center_y)**2)
ati_dist_corr = np.corrcoef(atis_array, distances)[0, 1]

# å¦‚æœæ˜¯ cluster_center ç­–ç•¥ï¼Œè¨ˆç®—åˆ°èšé¡ä¸­å¿ƒçš„è·é›¢ç›¸é—œæ€§
ati_cluster_dist_corr = None
if STRATEGY == 'cluster_center':
    # è¨ˆç®—æ¯å€‹å“ç‰Œåˆ°å…¶èšé¡ä¸­å¿ƒçš„è·é›¢
    distances_to_cluster_center = []
    for i in range(len(brand_list)):
        cluster_id = clusters[i]
        cluster_center = cluster_centers_2d[cluster_id]
        dist = np.linalg.norm(coordinates_2d[i] - cluster_center)
        distances_to_cluster_center.append(dist)
    ati_cluster_dist_corr = np.corrcoef(atis_array, distances_to_cluster_center)[0, 1]

print(f"   âœ“ é™ç¶­å®Œæˆ")
print(f"   âœ“ ATI èˆ‡ X åº§æ¨™ç›¸é—œæ€§: {ati_x_corr:.3f}")
print(f"   âœ“ ATI èˆ‡ Y åº§æ¨™ç›¸é—œæ€§: {ati_y_corr:.3f}")
print(f"   âœ“ ATI èˆ‡åˆ°ä¸­å¿ƒè·é›¢ç›¸é—œæ€§: {ati_dist_corr:.3f}")
if ati_cluster_dist_corr is not None:
    print(f"   âœ“ ATI èˆ‡åˆ°èšé¡ä¸­å¿ƒè·é›¢ç›¸é—œæ€§: {ati_cluster_dist_corr:.3f} (è² å€¼è¡¨ç¤ºé«˜ ATI æ›´æ¥è¿‘èšé¡ä¸­å¿ƒ)")

# 6. K-means èšé¡ï¼ˆå¦‚æœç­–ç•¥ä¸æ˜¯ cluster_centerï¼Œå‰‡åœ¨é€™è£¡é€²è¡Œï¼‰
if STRATEGY != 'cluster_center':
    print("\n6ï¸âƒ£ åŸ·è¡Œ K-means èšé¡...")
    n_clusters = 6
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(embeddings_scaled)
    
    print(f"   âœ“ èšé¡å®Œæˆï¼Œåˆ†æˆ {n_clusters} å€‹ç¾¤çµ„")
    for i in range(n_clusters):
        count = np.sum(clusters == i)
        print(f"     - ç¾¤çµ„ {i+1}: {count} å€‹å“ç‰Œ")
else:
    # cluster_center ç­–ç•¥å·²ç¶“åœ¨é™ç¶­æ­¥é©Ÿä¸­é€²è¡Œäº†èšé¡
    n_clusters = len(np.unique(clusters))
    print(f"\n6ï¸âƒ£ èšé¡å·²åœ¨é™ç¶­æ­¥é©Ÿä¸­å®Œæˆï¼Œåˆ†æˆ {n_clusters} å€‹ç¾¤çµ„")
    for i in range(n_clusters):
        count = np.sum(clusters == i)
        print(f"     - ç¾¤çµ„ {i+1}: {count} å€‹å“ç‰Œ")

# 7. è¨ˆç®—è¶¨åŒåº¦
print("\n7ï¸âƒ£ è¨ˆç®—è¶¨åŒåº¦...")
from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import cosine_similarity

embeddings_normalized = normalize(embeddings_matrix, norm='l2')
similarity_matrix = cosine_similarity(embeddings_normalized)

n_brands = len(brand_list)
avg_similarity = 0
count = 0
for i in range(n_brands):
    for j in range(i+1, n_brands):
        avg_similarity += similarity_matrix[i, j]
        count += 1
avg_similarity = avg_similarity / count if count > 0 else 0

# è¶¨åŒåº¦æŒ‡æ•¸ï¼šç›´æ¥ä½¿ç”¨å¹³å‡ç›¸ä¼¼åº¦ï¼ˆç¯„åœ [0, 1] æ˜ å°„åˆ° [0, 100]ï¼‰
convergence_index = avg_similarity * 100

print(f"   âœ“ å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
print(f"   âœ“ è¶¨åŒåº¦æŒ‡æ•¸: {convergence_index:.2f}%")

# 8. æº–å‚™è¼¸å‡ºè³‡æ–™
print("\n8ï¸âƒ£ æº–å‚™è¼¸å‡ºè³‡æ–™...")
output_data = {
    'method': 'ati_aware',
    'strategy': STRATEGY,
    'embedding_dim': embeddings_matrix.shape[1],
    'reduction_method': reduction_method,
    'n_clusters': n_clusters,
    'ati_x_correlation': float(ati_x_corr),
    'ati_y_correlation': float(ati_y_corr),
    'ati_distance_correlation': float(ati_dist_corr),
    'ati_cluster_distance_correlation': float(ati_cluster_dist_corr) if ati_cluster_dist_corr is not None else None,
    'convergence_index': float(convergence_index),
    'avg_similarity': float(avg_similarity),
    'brands': []
}

for i, brand in enumerate(brand_list):
    output_data['brands'].append({
        'brand': brand,
        'x': float(coordinates_2d[i, 0]),
        'y': float(coordinates_2d[i, 1]),
        'cluster': int(clusters[i]),
        'n_posts': int(brand_info[brand]['n_posts']),
        'ATI_final_mean': float(brand_info[brand]['ATI_final_mean']),
        'DS_final_mean': float(brand_info[brand]['DS_final_mean']),
        'y_mean': float(brand_info[brand]['y_mean']),
    })

# 9. å„²å­˜æª”æ¡ˆ
print(f"\n9ï¸âƒ£ å„²å­˜çµæœåˆ° {OUTPUT_FILE}...")
with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=2)

print(f"   âœ“ æª”æ¡ˆå·²å„²å­˜")
print(f"\nâœ… å®Œæˆï¼å…±è™•ç† {len(brand_list)} å€‹å“ç‰Œ")
print(f"   - é™ç¶­æ–¹æ³•: {reduction_method}")
print(f"   - ç­–ç•¥: {STRATEGY}")
print(f"   - ATI èˆ‡åˆ°ä¸­å¿ƒè·é›¢ç›¸é—œæ€§: {ati_dist_corr:.3f}")
