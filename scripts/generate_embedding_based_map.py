"""
ä½¿ç”¨åŸå§‹ CLIP embedding ç”¢ç”Ÿå“ç‰Œå®šä½åœ–å’Œèšé¡

é€™å€‹è…³æœ¬æœƒï¼š
1. è®€å– modal_embeddings_v2.npz ä¸­çš„ embedding
2. ç‚ºæ¯å€‹å“ç‰Œè¨ˆç®—å¹³å‡ embeddingï¼ˆçµåˆ text å’Œ imageï¼‰
3. ä½¿ç”¨ PCA/t-SNE é™ç¶­åˆ° 2D
4. ä½¿ç”¨ K-means é€²è¡Œèšé¡
5. è¼¸å‡º CSV æª”æ¡ˆä¾›å‰ç«¯ä½¿ç”¨
"""

import numpy as np
import pandas as pd
import json
from pathlib import Path
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("âš ï¸  UMAP æœªå®‰è£ï¼Œå°‡ä½¿ç”¨ PCAã€‚å®‰è£æ–¹å¼: pip install umap-learn")
import warnings
warnings.filterwarnings('ignore')

# è¨­å®šè·¯å¾‘
ROOT = Path(__file__).parent.parent
EMBEDDING_FILE = ROOT / 'çµæœ' / 'modal_embeddings_v2.npz'
TEST_CSV = ROOT / 'çµæœ' / 'ati_test_per_post.csv'
TRAIN_CSV = ROOT / 'çµæœ' / 'ati_train_per_post.csv'
OUTPUT_FILE = ROOT / 'src' / 'data' / 'generated' / 'embedding_based_map.json'

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)

print("ğŸ“Š é–‹å§‹ç”¢ç”ŸåŸºæ–¼ embedding çš„å“ç‰Œå®šä½åœ–...")

# 1. è®€å– embedding
print("\n1ï¸âƒ£ è®€å– embedding æª”æ¡ˆ...")
data = np.load(EMBEDDING_FILE, allow_pickle=True)
cap_test = data['cap_test']  # (590, 512)
ocr_test = data['ocr_test']  # (590, 512)
img_test = data['img_test']  # (590, 512)

# è®€å–è¨“ç·´é›† embeddingï¼ˆå¦‚æœå­˜åœ¨ï¼‰
cap_train = data.get('cap_train', None)
ocr_train = data.get('ocr_train', None)
img_train = data.get('img_train', None)

print(f"   âœ“ æ¸¬è©¦é›† embedding å½¢ç‹€:")
print(f"     - Caption: {cap_test.shape}")
print(f"     - OCR: {ocr_test.shape}")
print(f"     - Image: {img_test.shape}")

if cap_train is not None:
    print(f"   âœ“ è¨“ç·´é›† embedding å½¢ç‹€:")
    print(f"     - Caption: {cap_train.shape}")
    print(f"     - OCR: {ocr_train.shape}")
    print(f"     - Image: {img_train.shape}")

# 2. è®€å–å“ç‰Œè³‡æ–™
print("\n2ï¸âƒ£ è®€å–å“ç‰Œè³‡æ–™...")
df_test = pd.read_csv(TEST_CSV)
print(f"   âœ“ æ¸¬è©¦é›†: {len(df_test)} ç¯‡è²¼æ–‡ï¼Œ{df_test['brand'].nunique()} å€‹å“ç‰Œ")

# è®€å–è¨“ç·´é›†è³‡æ–™ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
df_train = None
if TRAIN_CSV.exists():
    df_train = pd.read_csv(TRAIN_CSV)
    print(f"   âœ“ è¨“ç·´é›†: {len(df_train)} ç¯‡è²¼æ–‡ï¼Œ{df_train['brand'].nunique()} å€‹å“ç‰Œ")
    
    # åˆä½µæ¸¬è©¦é›†å’Œè¨“ç·´é›†
    df_all = pd.concat([df_test, df_train], ignore_index=True)
    print(f"   âœ“ åˆä½µå¾Œ: {len(df_all)} ç¯‡è²¼æ–‡ï¼Œ{df_all['brand'].nunique()} å€‹å“ç‰Œ")
else:
    df_all = df_test
    print(f"   âš ï¸  è¨“ç·´é›†æª”æ¡ˆä¸å­˜åœ¨ï¼Œåƒ…ä½¿ç”¨æ¸¬è©¦é›†")

# 3. è¨ˆç®—æ¯å€‹å“ç‰Œçš„å¹³å‡ embedding
print("\n3ï¸âƒ£ è¨ˆç®—å“ç‰Œç´š embedding...")

# çµ„åˆæ¸¬è©¦é›†çš„ text embeddingï¼ˆcaption + OCRï¼‰
text_emb_test = np.hstack([cap_test, ocr_test])  # (590, 1024)
full_emb_test = np.hstack([text_emb_test, img_test])  # (590, 1536)

# çµ„åˆè¨“ç·´é›†çš„ text embeddingï¼ˆå¦‚æœå­˜åœ¨ï¼‰
full_emb_train = None
if cap_train is not None and ocr_train is not None and img_train is not None:
    text_emb_train = np.hstack([cap_train, ocr_train])
    full_emb_train = np.hstack([text_emb_train, img_train])
    # åˆä½µæ¸¬è©¦é›†å’Œè¨“ç·´é›†çš„ embedding
    full_emb = np.vstack([full_emb_test, full_emb_train])  # (total, 1536)
    print(f"   âœ“ åˆä½µ embedding: {full_emb.shape}")
else:
    full_emb = full_emb_test
    print(f"   âš ï¸  åƒ…ä½¿ç”¨æ¸¬è©¦é›† embedding: {full_emb.shape}")

brands = df_all['brand'].unique()
brand_embeddings = {}
brand_info = {}

for brand in brands:
    # æ‰¾å‡ºè©²å“ç‰Œåœ¨æ¸¬è©¦é›†å’Œè¨“ç·´é›†ä¸­çš„ç´¢å¼•
    brand_mask_test = df_test['brand'] == brand
    brand_indices_test = np.where(brand_mask_test)[0]
    
    brand_indices_train = []
    if df_train is not None:
        brand_mask_train = df_train['brand'] == brand
        brand_indices_train = np.where(brand_mask_train)[0] + len(df_test)  # åŠ ä¸Šæ¸¬è©¦é›†çš„åç§»é‡
    
    all_indices = list(brand_indices_test) + list(brand_indices_train)
    
    if len(all_indices) == 0:
        continue
    
    # è¨ˆç®—è©²å“ç‰Œæ‰€æœ‰è²¼æ–‡çš„å¹³å‡ embeddingï¼ˆåŒ…å«æ¸¬è©¦é›†å’Œè¨“ç·´é›†ï¼‰
    brand_emb = full_emb[all_indices].mean(axis=0)  # (1536,)
    brand_embeddings[brand] = brand_emb
    
    # å„²å­˜å“ç‰Œçš„å…¶ä»–è³‡è¨Šï¼ˆå¾åˆä½µå¾Œçš„è³‡æ–™è¨ˆç®—ï¼‰
    brand_posts = df_all[df_all['brand'] == brand]
    brand_info[brand] = {
        'n_posts': len(brand_posts),
        'ATI_final_mean': brand_posts['ATI_final'].mean(),
        'DS_final_mean': brand_posts['DS_final'].mean(),
        'y_mean': brand_posts['y'].mean(),
        'text_ATI_mean': brand_posts['text_ATI'].mean(),
        'image_ATI_mean': brand_posts['image_ATI'].mean(),
    }

print(f"   âœ“ è¨ˆç®—å®Œæˆï¼Œå…± {len(brand_embeddings)} å€‹å“ç‰Œ")
print(f"   âœ“ Embedding ç¶­åº¦: {full_emb.shape[1]}")

# 4. æº–å‚™é™ç¶­è³‡æ–™
print("\n4ï¸âƒ£ æº–å‚™é™ç¶­...")
brand_list = list(brand_embeddings.keys())
embeddings_matrix = np.array([brand_embeddings[brand] for brand in brand_list])

# æ¨™æº–åŒ–ï¼ˆé‡è¦ï¼ï¼‰
scaler = StandardScaler()
embeddings_scaled = scaler.fit_transform(embeddings_matrix)

print(f"   âœ“ æ¨™æº–åŒ–å®Œæˆï¼Œå½¢ç‹€: {embeddings_scaled.shape}")

# 5. é™ç¶­åˆ° 2Dï¼ˆå„ªå…ˆä½¿ç”¨ UMAPï¼Œæ›´ç›´è§€ä¸”ä¿ç•™æ›´å¤šçµæ§‹ä¿¡æ¯ï¼‰
print("\n5ï¸âƒ£ åŸ·è¡Œé™ç¶­...")
reduction_method = "PCA"  # é è¨­å€¼

# å„ªå…ˆä½¿ç”¨ UMAPï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå› ç‚ºå®ƒèƒ½æ›´å¥½åœ°ä¿ç•™å±€éƒ¨å’Œå…¨å±€çµæ§‹
if UMAP_AVAILABLE:
    print("   ğŸ¯ ä½¿ç”¨ UMAP é™ç¶­ï¼ˆæ¨è–¦ï¼šèƒ½æ›´å¥½åœ°ä¿ç•™æ•¸æ“šçµæ§‹ï¼‰...")
    try:
        # UMAP åƒæ•¸ï¼š
        # n_neighbors: æ§åˆ¶å±€éƒ¨çµæ§‹çš„ä¿ç•™ï¼ˆè¼ƒå°å€¼æ›´é—œæ³¨å±€éƒ¨ï¼Œè¼ƒå¤§å€¼æ›´é—œæ³¨å…¨å±€ï¼‰
        # min_dist: æ§åˆ¶é»ä¹‹é–“çš„ç·Šå¯†åº¦ï¼ˆ0.0-1.0ï¼Œè¼ƒå°å€¼æ›´ç·Šå¯†ï¼‰
        # metric: è·é›¢åº¦é‡æ–¹å¼
        reducer = umap.UMAP(
            n_components=2,
            n_neighbors=15,  # å°æ–¼ 63 å€‹å“ç‰Œï¼Œ15 æ˜¯å€‹åˆç†çš„å€¼
            min_dist=0.1,     # å…è¨±é»ä¹‹é–“æœ‰é©åº¦çš„è·é›¢ï¼Œä¸æœƒå¤ªç·Šå¯†
            metric='cosine',   # ä½¿ç”¨é¤˜å¼¦è·é›¢ï¼Œé©åˆ embedding
            random_state=42,
            verbose=False
        )
        coordinates_2d = reducer.fit_transform(embeddings_scaled)
        reduction_method = "UMAP"
        print(f"   âœ“ UMAP å®Œæˆ")
        print(f"   âœ“ ä½¿ç”¨é¤˜å¼¦è·é›¢åº¦é‡ï¼Œä¿ç•™å±€éƒ¨å’Œå…¨å±€çµæ§‹")
        
        # è¨ˆç®—è§£é‡‹è®Šç•°ï¼ˆUMAP ä¸ç›´æ¥æä¾›ï¼Œä½†å¯ä»¥è¨ˆç®—ä¿ç•™çš„è·é›¢ä¿¡æ¯ï¼‰
        # é€™è£¡æˆ‘å€‘è¨ˆç®—åŸå§‹è·é›¢å’Œé™ç¶­å¾Œè·é›¢çš„ç›¸é—œæ€§
        from scipy.spatial.distance import pdist, squareform
        original_distances = pdist(embeddings_scaled, metric='cosine')
        reduced_distances = pdist(coordinates_2d, metric='euclidean')
        from scipy.stats import spearmanr
        correlation, _ = spearmanr(original_distances, reduced_distances)
        print(f"   âœ“ è·é›¢ä¿ç•™ç›¸é—œæ€§: {correlation:.2%}")
    except Exception as e:
        print(f"   âš ï¸  UMAP å¤±æ•—: {e}ï¼Œå›é€€åˆ° PCA")
        reduction_method = "PCA"
        # å›é€€åˆ° PCA
        n_components = min(50, len(brand_list) - 1, embeddings_scaled.shape[1])
        pca = PCA(n_components=n_components)
        embeddings_pca = pca.fit_transform(embeddings_scaled)
        pca_2d = PCA(n_components=2)
        coordinates_2d = pca_2d.fit_transform(embeddings_pca)
        explained_variance = pca_2d.explained_variance_ratio_.sum() * pca.explained_variance_ratio_[:n_components].sum()
        print(f"   âœ“ PCA å®Œæˆ")
        print(f"   âœ“ å‰å…©å€‹ä¸»æˆåˆ†è§£é‡‹è®Šç•°: {explained_variance:.2%}")
else:
    # ä½¿ç”¨ PCAï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
    print("   ğŸ“Š ä½¿ç”¨ PCA é™ç¶­...")
    n_components = min(50, len(brand_list) - 1, embeddings_scaled.shape[1])
    pca = PCA(n_components=n_components)
    embeddings_pca = pca.fit_transform(embeddings_scaled)
    pca_2d = PCA(n_components=2)
    coordinates_2d = pca_2d.fit_transform(embeddings_pca)
    explained_variance = pca_2d.explained_variance_ratio_.sum() * pca.explained_variance_ratio_[:n_components].sum()
    print(f"   âœ“ PCA å®Œæˆ")
    print(f"   âœ“ å‰å…©å€‹ä¸»æˆåˆ†è§£é‡‹è®Šç•°: {explained_variance:.2%}")

# è¨ˆç®—è§£é‡‹è®Šç•°ï¼ˆç”¨æ–¼è¼¸å‡ºï¼ŒUMAP æ™‚ä½¿ç”¨ç›¸é—œæ€§ï¼‰
if reduction_method == "PCA":
    explained_variance_value = explained_variance
else:
    explained_variance_value = correlation if 'correlation' in locals() else 0.0

# 6. K-means èšé¡
print("\n6ï¸âƒ£ åŸ·è¡Œ K-means èšé¡...")
n_clusters = 6  # ä½¿ç”¨ 6 å€‹ç¾¤çµ„
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
clusters = kmeans.fit_predict(embeddings_scaled)

print(f"   âœ“ èšé¡å®Œæˆï¼Œåˆ†æˆ {n_clusters} å€‹ç¾¤çµ„")
for i in range(n_clusters):
    count = np.sum(clusters == i)
    print(f"     - ç¾¤çµ„ {i+1}: {count} å€‹å“ç‰Œ")

# 7. è¨ˆç®—åŸºæ–¼ embedding çš„è¶¨åŒåº¦
print("\n7ï¸âƒ£ è¨ˆç®—è¶¨åŒåº¦...")

# ä½¿ç”¨åŸå§‹ embeddingï¼ˆæœªæ¨™æº–åŒ–ï¼‰é€²è¡Œ L2 æ­£è¦åŒ–
# é€™æ¨£å¯ä»¥æ›´æº–ç¢ºåœ°åæ˜ å“ç‰Œåœ¨èªæ„ç©ºé–“ä¸­çš„ç›¸ä¼¼åº¦
from sklearn.preprocessing import normalize
embeddings_normalized = normalize(embeddings_matrix, norm='l2')

# æ–¹æ³•1: è¨ˆç®—æ‰€æœ‰å“ç‰Œ embedding çš„å¹³å‡ä¸­å¿ƒé»
center_embedding = embeddings_normalized.mean(axis=0)
# æ­£è¦åŒ–ä¸­å¿ƒé»
center_embedding = center_embedding / (np.linalg.norm(center_embedding) + 1e-9)

# è¨ˆç®—æ¯å€‹å“ç‰Œåˆ°ä¸­å¿ƒçš„è·é›¢ï¼ˆåœ¨æ­£è¦åŒ–ç©ºé–“ä¸­ï¼‰
distances_to_center = []
for i, brand in enumerate(brand_list):
    # ä½¿ç”¨ cosine distance = 1 - cosine similarity
    cosine_sim = np.dot(embeddings_normalized[i], center_embedding)
    cosine_dist = 1 - cosine_sim
    distances_to_center.append(cosine_dist)

avg_distance = np.mean(distances_to_center)
std_distance = np.std(distances_to_center)

# æ–¹æ³•2: è¨ˆç®—å“ç‰Œä¹‹é–“çš„ç›¸ä¼¼åº¦çŸ©é™£ï¼ˆcosine similarityï¼‰
from sklearn.metrics.pairwise import cosine_similarity
similarity_matrix = cosine_similarity(embeddings_normalized)
# å–ä¸Šä¸‰è§’ï¼ˆä¸åŒ…æ‹¬å°è§’ç·šï¼‰ï¼Œè¨ˆç®—å¹³å‡ç›¸ä¼¼åº¦
n_brands = len(brand_list)
avg_similarity = 0
count = 0
for i in range(n_brands):
    for j in range(i+1, n_brands):
        avg_similarity += similarity_matrix[i, j]
        count += 1
avg_similarity = avg_similarity / count if count > 0 else 0

# è¶¨åŒåº¦æŒ‡æ•¸ï¼šç›´æ¥ä½¿ç”¨å¹³å‡ç›¸ä¼¼åº¦ï¼ˆç¯„åœ [0, 1] æ˜ å°„åˆ° [0, 100]ï¼‰
# ç›¸ä¼¼åº¦ç¯„åœåœ¨ 0 åˆ° 1 ä¹‹é–“ï¼ˆL2 æ­£è¦åŒ–å¾Œçš„ CLIP embeddingï¼‰
# å¦‚æœç›¸ä¼¼åº¦å¾ˆä½ï¼ˆæ¥è¿‘ 0ï¼‰ï¼Œè¡¨ç¤ºå“ç‰Œå·®ç•°å¾ˆå¤§ï¼ˆå¤šå…ƒï¼‰
# å¦‚æœç›¸ä¼¼åº¦å¾ˆé«˜ï¼ˆæ¥è¿‘ 1ï¼‰ï¼Œè¡¨ç¤ºå“ç‰Œå¾ˆç›¸ä¼¼ï¼ˆè¶¨åŒï¼‰
convergence_index = avg_similarity * 100

print(f"   âœ“ å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
print(f"   âœ“ è¶¨åŒåº¦æŒ‡æ•¸: {convergence_index:.2f}%")
print(f"   âœ“ å¹³å‡è·é›¢: {avg_distance:.4f}")
print(f"   âœ“ è·é›¢æ¨™æº–å·®: {std_distance:.4f}")

# 8. æº–å‚™è¼¸å‡ºè³‡æ–™
print("\n8ï¸âƒ£ æº–å‚™è¼¸å‡ºè³‡æ–™...")
output_data = {
    'method': 'embedding_based',
    'embedding_dim': full_emb.shape[1],
    'reduction_method': reduction_method,
    'n_clusters': n_clusters,
    'explained_variance': float(explained_variance_value) if reduction_method == "PCA" else float(explained_variance_value),
    'convergence_index': float(convergence_index),
    'avg_similarity': float(avg_similarity),
    'avg_distance': float(avg_distance),
    'std_distance': float(std_distance),
    'brands': []
}

for i, brand in enumerate(brand_list):
    output_data['brands'].append({
        'brand': brand,
        'x': float(coordinates_2d[i, 0]),
        'y': float(coordinates_2d[i, 1]),
        'cluster': int(clusters[i]),
        'n_posts': int(brand_info[brand]['n_posts']),
        'ATI_final_mean': float(brand_info[brand]['ATI_final_mean']),
        'DS_final_mean': float(brand_info[brand]['DS_final_mean']),
        'y_mean': float(brand_info[brand]['y_mean']),
        'text_ATI_mean': float(brand_info[brand]['text_ATI_mean']),
        'image_ATI_mean': float(brand_info[brand]['image_ATI_mean']),
    })

# 9. å„²å­˜æª”æ¡ˆ
print(f"\n9ï¸âƒ£ å„²å­˜çµæœåˆ° {OUTPUT_FILE}...")
with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, ensure_ascii=False, indent=2)

print(f"   âœ“ æª”æ¡ˆå·²å„²å­˜")
print(f"\nâœ… å®Œæˆï¼å…±è™•ç† {len(brand_list)} å€‹å“ç‰Œ")
print(f"   - é™ç¶­æ–¹æ³•: {reduction_method}")
print(f"   - èšé¡æ•¸: {n_clusters}")
if reduction_method == "PCA":
    print(f"   - è§£é‡‹è®Šç•°: {explained_variance_value:.2%}")
else:
    print(f"   - è·é›¢ä¿ç•™ç›¸é—œæ€§: {explained_variance_value:.2%}")

