{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ee_7tSo4mAFg","outputId":"5a1afb01-e5ec-461c-940a-9be8446ffac4","executionInfo":{"status":"ok","timestamp":1765195217110,"user_tz":-480,"elapsed":271607,"user":{"displayName":"孫旖旎","userId":"14812993266795048665"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["第一次在這個 runtime 安裝套件，會比較久，請耐心等候…\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\n","版本確認：\n","numpy: 2.0.2\n","pandas: 2.2.2\n","scipy: 1.16.3\n","sklearn: 1.6.1\n","\n"," 套件安裝完成，可以直接往下跑 Cell 2。\n"]}],"source":["# =========================\n","# ==== Cell 1 : 套件安裝 ====\n","# =========================\n","import os\n","\n","SETUP_FLAG = \"/content/.ati_env_ready\"\n","\n","if not os.path.exists(SETUP_FLAG):\n","    print(\"第一次在這個 runtime 安裝套件，會比較久，請耐心等候…\")\n","\n","    !pip -q install --upgrade pip\n","    # 不再動 numpy/pandas/scipy/sklearn，使用 Colab 內建版本\n","    !pip -q install \"pillow==10.4.0\" \"tqdm==4.66.5\" \"regex==2024.9.11\" \"emoji==2.12.1\"\n","    !pip -q install \"opencv-python-headless==4.10.0.84\" \"easyocr==1.7.1\" \"openpyxl==3.1.5\"\n","    !pip -q install \"transformers==4.44.2\"\n","    !pip -q install \"torch==2.4.1\" \"torchvision==0.19.1\" \"torchaudio==2.4.1\"\n","\n","    # 做個記號，之後這個 runtime 不用再安裝\n","    open(SETUP_FLAG, \"w\").close()\n","\n","    # 看一下實際版本（確認沒有壞掉）\n","    import numpy, pandas, scipy, sklearn\n","    print(\"\\n版本確認：\")\n","    print(\"numpy:\", numpy.__version__)\n","    print(\"pandas:\", pandas.__version__)\n","    print(\"scipy:\", scipy.__version__)\n","    print(\"sklearn:\", sklearn.__version__)\n","    print(\"\\n 套件安裝完成，可以直接往下跑 Cell 2。\")\n","else:\n","    print(\" 這個 runtime 已經安裝過相依套件了，略過 pip install。\")\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uoD6LdQyPH5C","outputId":"2ea6c20f-a422-4de4-ee26-7d7214eeaafe","executionInfo":{"status":"ok","timestamp":1765195806946,"user_tz":-480,"elapsed":2929,"user":{"displayName":"孫旖旎","userId":"14812993266795048665"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# =========================\n","# ==== Cell 2: 掛載雲端 ====\n","# =========================\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 設定路徑\n","BASE_DIR = '/content/drive/MyDrive/output_split_73'\n","TRAIN_CSV = f'{BASE_DIR}/with_rel_paths_train_posts.csv'\n","TEST_CSV  = f'{BASE_DIR}/with_rel_paths_test_posts.csv'\n","BRAND_FOLLOWERS_CSV = f'{BASE_DIR}/brand_followers.csv'\n","BRAND_FOLLOWERS_XLSX = f'{BASE_DIR}/brand_followers.xlsx'\n","\n","IMG_TRAIN_DIR = f'{BASE_DIR}/train'\n","IMG_TEST_DIR  = f'{BASE_DIR}/test'\n","\n","# 輸出目錄\n","OUT_DIR = f'{BASE_DIR}/outputs'\n","import os\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","CACHE_DIR = f'{BASE_DIR}/cache'\n","os.makedirs(CACHE_DIR, exist_ok=True)\n"]},{"cell_type":"code","source":["!ls /content/drive/Shareddrives"],"metadata":{"id":"OwZ82qJT5tIZ","executionInfo":{"status":"ok","timestamp":1765196355658,"user_tz":-480,"elapsed":141,"user":{"displayName":"孫旖旎","userId":"14812993266795048665"}},"outputId":"4a627bd1-d4b9-41a3-832d-eb997b9dad6d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access '/content/drive/Shareddrives': No such file or directory\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"id":"85Zx5M-6PJJI","executionInfo":{"status":"ok","timestamp":1765195808993,"user_tz":-480,"elapsed":18,"user":{"displayName":"孫旖旎","userId":"14812993266795048665"}}},"outputs":[],"source":["# ===================================\n","# ==== Cell 3: 匯入、函式 ========\n","# ===================================\n","import os, ast, re, json, math, gc, unicodedata, random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from PIL import Image, UnidentifiedImageError\n","import torch\n","from transformers import CLIPProcessor, CLIPModel\n","from sklearn.cluster import KMeans, MiniBatchKMeans\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression\n","\n","# ---- 有待調整!! ----\n","K_CLUSTERS = 6\n","TAU = 0.07                 # diversity 用的 softmax 溫度\n","OCR_MAX_IMAGES = 1         # 每篇最多 OCR 幾張\n","IMG_MAX_IMAGES = 1         # 每篇最多取幾張圖算影像嵌入\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED)\n","\n","# ---- 函式 ----\n","def safe_read_followers():\n","    if os.path.exists(BRAND_FOLLOWERS_CSV):\n","        df = pd.read_csv(BRAND_FOLLOWERS_CSV)\n","        return df\n","    elif os.path.exists(BRAND_FOLLOWERS_XLSX):\n","        df = pd.read_excel(BRAND_FOLLOWERS_XLSX)\n","        return df\n","    else:\n","        raise FileNotFoundError(\"找不到 brand_followers.csv 或 brand_followers.xlsx，請放到 MyDrive/project/output_split_73/\")\n","\n","def parse_rel_img_paths(cell):\n","    \"\"\"\n","    支援幾種常見格式：\n","    - \"['a.jpg', 'b.jpg']\"\n","    - 'a.jpg|b.jpg'\n","    - 'a.jpg,b.jpg'\n","    - 單一路徑字串\n","    \"\"\"\n","    if pd.isna(cell):\n","        return []\n","    s = str(cell).strip()\n","    if s == '' or s.lower() == 'nan':\n","        return []\n","    # 嘗試 list 字串\n","    try:\n","        val = ast.literal_eval(s)\n","        if isinstance(val, list):\n","            return [str(x).strip() for x in val]\n","    except Exception:\n","        pass\n","    # 以常見分隔符切\n","    if '|' in s:\n","        return [x.strip() for x in s.split('|') if x.strip()]\n","    if ',' in s:\n","        return [x.strip() for x in s.split(',') if x.strip()]\n","    return [s]\n","\n","EMOJI_PATTERN = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n","def count_emojis(text):\n","    if not isinstance(text, str): return 0\n","    return len(EMOJI_PATTERN.findall(text))\n","\n","def count_pattern(text, pattern):\n","    if not isinstance(text, str): return 0\n","    return len(re.findall(pattern, text))\n","\n","def has_any(text, keywords):\n","    if not isinstance(text, str): return 0\n","    low = text.lower()\n","    return int(any(kw in low for kw in keywords))\n","\n","def l2_normalize(v, eps=1e-9):\n","    v = np.asarray(v, dtype=np.float32)\n","    n = np.linalg.norm(v) + eps\n","    return v / n\n","\n","def cosine_sim(a, b, eps=1e-9):\n","    a = a / (np.linalg.norm(a) + eps)\n","    b = b / (np.linalg.norm(b) + eps)\n","    return float(np.dot(a, b))\n","\n","def softmax(x, tau=1.0):\n","    x = np.asarray(x, dtype=np.float32)\n","    z = (x - np.max(x)) / max(tau, 1e-8)\n","    e = np.exp(z)\n","    return e / (e.sum() + 1e-9)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"JpEu049FPK2B","outputId":"d9cad922-11c5-403a-f82f-77ecd5121635","executionInfo":{"status":"error","timestamp":1765196006748,"user_tz":-480,"elapsed":53,"user":{"displayName":"孫旖旎","userId":"14812993266795048665"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/output_split_73/with_rel_paths_train_posts.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1585079692.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ==== Cell 4 讀資料 ====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ==================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_df\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/output_split_73/with_rel_paths_train_posts.csv'"]}],"source":["# ==================================\n","# ==== Cell 4 讀資料 ====\n","# ==================================\n","train_df = pd.read_csv(TRAIN_CSV)\n","test_df  = pd.read_csv(TEST_CSV)\n","\n","# 指定欄位名\n","brand_col   = 'brand'      # IG 名稱\n","caption_col = 'sum'        # 貼文文字\n","time_col    = 'time_converted'  # 發文時間（形如 \"2025-05-01 20:52:16\"）\n","\n","# 讀 followers（優先 xlsx，否則 csv）\n","def load_followers_table():\n","    if os.path.exists(BRAND_FOLLOWERS_XLSX):\n","        df = pd.read_excel(BRAND_FOLLOWERS_XLSX)\n","    elif os.path.exists(BRAND_FOLLOWERS_CSV):\n","        df = pd.read_csv(BRAND_FOLLOWERS_CSV)\n","    else:\n","        raise FileNotFoundError(\"找不到 brand_followers.xlsx 或 brand_followers.csv，請放到 MyDrive/project/output_split_73/\")\n","    required = {'brand','followers'}\n","    missing = required - set(df.columns)\n","    if missing:\n","        raise KeyError(f\"brand_followers 檔案缺少欄位：{missing}\")\n","    return df[['brand','followers']].copy()\n","\n","followers_df = load_followers_table()\n","\n","# 合併 followers\n","train = train_df.merge(followers_df, on='brand', how='left')\n","test  = test_df.merge(followers_df,  on='brand', how='left')\n","train['followers'] = train['followers'].fillna(0.0).astype(float)\n","test['followers']  = test['followers'].fillna(0.0).astype(float)\n","\n","# ========= 新增：只保留「非影片」的貼文 =========\n","def filter_only_images(df, split_name=\"train\"):\n","    if 'is_video' not in df.columns:\n","        print(f\"[警告] {split_name} 找不到 is_video 欄位，暫時不過濾影片貼文。\")\n","        return df\n","\n","    # 先把 is_video 統一轉成布林\n","    col = df['is_video']\n","    # 很多時候 CSV 會是字串 \"True\"/\"False\"\n","    is_video_bool = (\n","        col.astype(str)\n","           .str.strip()\n","           .str.lower()\n","           .map({'true': True, 'false': False})\n","           .fillna(False)        # 看不懂的當成不是影片\n","           .astype(bool)\n","    )\n","\n","    before = len(df)\n","    df_img = df[~is_video_bool].copy()\n","    after = len(df_img)\n","    print(f\"[{split_name}] 過濾影片貼文：原本 {before} 筆，保留 {after} 筆（純圖片貼文）\")\n","    return df_img\n","\n","train = filter_only_images(train, \"train\")\n","test  = filter_only_images(test,  \"test\")\n","# ==============================================\n","\n","# 必要欄位檢查\n","required_cols = ['brand','sum','count_like','count_comment','rel_img_paths','time_converted']\n","for col in required_cols:\n","    if col not in train.columns or col not in test.columns:\n","        raise KeyError(f\"找不到必要欄位：{col}，請確認 CSV。\")\n","\n","# 快速看一下欄位\n","print(\"Train columns:\", list(train.columns)[:20], \"... total\", len(train.columns))\n","print(\"Test  columns:\", list(test.columns)[:20],  \"... total\", len(test.columns))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrbTB_P1PNlp","outputId":"fedd03e6-f8f6-409c-d562-b2dd64618695"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"]},{"name":"stdout","output_type":"stream","text":["Progress: |██████████████████████████████████████████████████| 100.0% Complete"]},{"name":"stderr","output_type":"stream","text":["WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"]},{"name":"stdout","output_type":"stream","text":["Progress: |██████████████████████████████████████████████████| 100.0% Complete"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n","/usr/local/lib/python3.12/dist-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path, map_location=device))\n"]}],"source":["# =======================================\n","# ==== Cell 5: OCR（帶快取）與讀圖工具 ====\n","# =======================================\n","import easyocr\n","reader = easyocr.Reader(['ch_tra','en'], gpu=True)  # 中文繁體 + 英文\n","\n","def ocr_single_image(img_path):\n","    try:\n","        result = reader.readtext(img_path, detail=0, paragraph=True)\n","        # 回傳合併後的文字\n","        text = \" \".join([r.strip() for r in result if isinstance(r, str)])\n","        return text\n","    except Exception:\n","        return \"\"\n","\n","def ocr_post(rel_paths, base_dir, cache_key):\n","    \"\"\"\n","    rel_paths: list[str] 相對路徑（或檔名）\n","    base_dir : train/test 圖片資料夾\n","    cache_key: 用 brand/timestamp/shortcode 等組出唯一鍵\n","    \"\"\"\n","    cache_file = os.path.join(CACHE_DIR, f'ocr_{cache_key}.json')\n","    if os.path.exists(cache_file):\n","        try:\n","            with open(cache_file,'r',encoding='utf-8') as f:\n","                return json.load(f).get('text','')\n","        except Exception:\n","            pass\n","\n","    texts = []\n","    cnt = 0\n","    for rp in rel_paths[:OCR_MAX_IMAGES]:\n","        img_path = os.path.join(base_dir, rp)\n","        if not os.path.exists(img_path):\n","            # 也許 rel 路徑本身就是完整檔名在 base_dir 下\n","            img_path2 = os.path.join(base_dir, os.path.basename(rp))\n","            if os.path.exists(img_path2):\n","                img_path = img_path2\n","            else:\n","                continue\n","        t = ocr_single_image(img_path)\n","        if t: texts.append(t)\n","        cnt += 1\n","    final_text = \" \".join(texts).strip()\n","    with open(cache_file,'w',encoding='utf-8') as f:\n","        json.dump({'text':final_text}, f, ensure_ascii=False)\n","    return final_text\n","\n","def load_image_for_clip(img_path, size_check=True):\n","    try:\n","        im = Image.open(img_path).convert('RGB')\n","        if size_check:\n","            # 避免極端大圖\n","            im.thumbnail((1024,1024))\n","        return im\n","    except (FileNotFoundError, UnidentifiedImageError, OSError):\n","        return None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":309,"referenced_widgets":["2089125947b04a3a928b1593373c8edc","3efe8d9aaedf4db482c6c9a266e8e5ae","993e33d3fde9441bad7120a9a2eefa15","c44b302036b7443f83827aa3a3452e8d","76ed02c4224244a39c36f6f47470cafc","e0bb7002d9974f68a7092939e0addb05","cb1e961dfdf4430fb6ef430154ee3766","5f91453e4fd0472bb3e377802ef5f387","9ee469f48de64a34900378f7c0e4a020","5ce1d004fed04c768e83990a64bfb9a5","80806f2b8d8d4b779755fe519a8f573e","9355e2a89610491285fb3c484914fd93","038be2dac2f540019a2fe530fbb7db02","63ed3dcfcad84ab18ba4be96ee2f05eb","3226c05f59fe4e268de0f4b444b8fbce","621e5f5f44bf47e79531e2519a40a8d7","5f3a8c4eb3ff481da7833c0a9833e4cc","fd1f9d1334d444ffa3398567b0e951dc","e416ff5a73564220bb05ad0457d543dd","3df5933cd5254398929bf5a15daae1f6","be0eb51ff0b84e9cb9f4f94a6458a10e","c50d2d48d9d649dfb70a533bcace8e6b","43a4c70835c249e58495f7f326c6c188","bf4f6e30f1914a7483407b7e924a39d4","dac3d7c3cb7c4a3db01ef4f65e8be6b5","f1827ebd3e54406a9dbf9475f8ef1b6a","c13ff9f13f43415a839b32c49c471b46","81805c5f925143d79b4b3d001fb0732f","92f6f542fbee4b0b9b893274f51061b0","bd795c3dbc7d4f1ebb97588b54e7e825","4190d8e52c0742fcb036b80ec0e84c32","cdead9ef4b764759a2cb0a244f9a911c","11545df95e6143f5b77f9866f2bf98cd","ffd1cf5150d1407fa39effba2627a579","5e5d4a0f136f40a3acd194272b119e18","eec1675c80234329a09a9a806bdfd77f","45e628433d87458eb822b46a821532df","d69579ba21a8435f92d8059018168876","f7d360c487b74e8ab5ea132bb683e7cc","b558c921c1ac48f0849701b52fe622bc","d81a1222838d4f38afd715a45a7f0494","531198654d8d4859ad5a3e6ccedaef26","a8382eb04b6e485c98fb3a984559ddd9","cae64c9b45ab4f6999936fc6c96007a7"]},"id":"zBrQ-NAlPPXJ","outputId":"b0d59c42-75d5-4602-bbe1-badd1d5b8191"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2089125947b04a3a928b1593373c8edc","version_major":2,"version_minor":0},"text/plain":["config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9355e2a89610491285fb3c484914fd93","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/753M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43a4c70835c249e58495f7f326c6c188","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ffd1cf5150d1407fa39effba2627a579","version_major":2,"version_minor":0},"text/plain":["vocab.txt: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["# =========================================\n","# ==== Cell 6 ：CLIP 模型與嵌入函式 ====\n","# =========================================\n","import torch\n","import numpy as np\n","from transformers import (\n","    CLIPModel, CLIPProcessor,\n","    ChineseCLIPModel, ChineseCLIPProcessor,\n",")\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# 可選 'chinese-clip' 或 'openai-clip'\n","MODEL_BACKEND = 'chinese-clip'   # 中文情境用這個\n","MODEL_ID_CN   = 'OFA-Sys/chinese-clip-vit-base-patch16'\n","MODEL_ID_EN   = 'openai/clip-vit-base-patch32'\n","\n","# 載入模型與處理器\n","if MODEL_BACKEND == 'chinese-clip':\n","    clip_model = ChineseCLIPModel.from_pretrained(MODEL_ID_CN).to(device)\n","    clip_processor = ChineseCLIPProcessor.from_pretrained(MODEL_ID_CN)\n","    PROJ_DIM = clip_model.config.projection_dim  # 一般為 512\n","else:\n","    clip_model = CLIPModel.from_pretrained(MODEL_ID_EN).to(device)\n","    clip_processor = CLIPProcessor.from_pretrained(MODEL_ID_EN)\n","    PROJ_DIM = clip_model.config.projection_dim  # 一般為 512\n","\n","import torch, numpy as np\n","\n","@torch.no_grad()\n","def embed_text_clip(texts, batch_size=64, max_length=64, device_override=None, use_fp16=True):\n","    \"\"\"\n","    分批把 texts 送入 CLIP 文字塔做向量化，並 L2 normalize。\n","    - batch_size: 建議先 64，不夠再降\n","    - max_length: 限制 tokenizer 輸入長度，避免超長 OCR 文字吃光記憶體\n","    - device_override: 需要時可強制 'cpu'（避免 GPU 爆）\n","    - use_fp16: GPU 上用 FP16 省記憶體\n","    \"\"\"\n","    dev = device_override if device_override is not None else device\n","    feats_all = []\n","    for i in range(0, len(texts), batch_size):\n","        chunk = texts[i:i+batch_size]\n","        # 先做字串裁切（避免極端長句），再交給 tokenizer 做 token 截斷\n","        chunk = [str(s)[:512] for s in chunk]\n","\n","        inputs = clip_processor(\n","            text=chunk, return_tensors='pt',\n","            padding=True, truncation=True, max_length=max_length\n","        ).to(dev)\n","\n","        # 半精度自動混合（僅 GPU 啟用）\n","        use_amp = (dev == 'cuda') and use_fp16\n","        ctx = torch.cuda.amp.autocast(enabled=use_amp, dtype=torch.float16) if use_amp else torch.autocast('cpu', enabled=False)\n","        with ctx:\n","            feats = clip_model.get_text_features(**inputs)  # [n, d]\n","\n","        arr = feats.detach().cpu().numpy()\n","        arr = arr / (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-9)\n","        feats_all.append(arr.astype(np.float32))\n","\n","        # 釋放顯示卡暫存\n","        if dev == 'cuda':\n","            del feats, inputs\n","            torch.cuda.empty_cache()\n","\n","    if len(feats_all) == 0:\n","        return np.zeros((0, PROJ_DIM), dtype=np.float32)\n","    return np.vstack(feats_all)\n","\n","@torch.no_grad()\n","def embed_text_clip_safe(texts):\n","    \"\"\"\n","    保險版：自動嘗試不同 batch_size；仍 OOM 就改跑 CPU。\n","    \"\"\"\n","    for bs in [128, 64, 32, 16, 8, 4, 2, 1]:\n","        try:\n","            return embed_text_clip(texts, batch_size=bs, max_length=64, device_override=None, use_fp16=True)\n","        except RuntimeError as e:\n","            if 'CUDA out of memory' in str(e):\n","                torch.cuda.empty_cache()\n","                continue\n","            else:\n","                raise\n","    # 退而求其次：改用 CPU（會慢，但能跑完）\n","    return embed_text_clip(texts, batch_size=64, max_length=64, device_override='cpu', use_fp16=False)\n","\n","\n","@torch.no_grad()\n","def embed_images_clip(pil_images):\n","    \"\"\"\n","    pil_images: list[PIL.Image]\n","    return: np.array [n, PROJ_DIM], L2-normalized\n","    \"\"\"\n","    if len(pil_images) == 0:\n","        return np.zeros((0, PROJ_DIM), dtype=np.float32)\n","    inputs = clip_processor(images=pil_images, return_tensors='pt').to(device)\n","    feats = clip_model.get_image_features(**inputs)  # [n, d]\n","    arr = feats.detach().cpu().numpy()\n","    arr = arr / (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-9)\n","    return arr.astype(np.float32)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUUCzhXrPQ-I"},"outputs":[],"source":["# =========================================================\n","# ==== Cell 7：數值型 Metadata 特徵函式 =========\n","# =========================================================\n","PROMO_WORDS  = ['折', '折扣', '%off', '% off', '促銷', '滿', '送', '優惠', '特價', '買一送一', '買一送二', '限時', '早鳥']\n","FLAVOR_WORDS = ['芒果', '草莓', '葡萄', '百香', '抹茶', '烏龍', '紅茶', '綠茶', '奶蓋', '珍珠', '椰果', '仙草']\n","HEALTH_WORDS = ['無糖', '微糖', '半糖', '少冰', '去冰', '低卡', '健康', '無添加']\n","CTA_WORDS    = ['快來', '立刻', '今天', '現在', '一起', '打卡', '留言', '分享', '抽獎']\n","PRICE_PAT    = r'(nt\\$|n\\$|\\$|元)\\s*\\d+'\n","PCT_PAT      = r'\\d+\\s*%'\n","\n","def build_numeric_features(df, text_col, ocr_col, time_col):\n","    \"\"\"\n","    df: 需要包含 text_col（caption）、ocr_col（OCR 文字）、time_col（time_converted）\n","    回傳: 僅數值欄位的 DataFrame（會在後續做 z-score）\n","    \"\"\"\n","    caps = df[text_col].fillna('').astype(str)\n","    ocrs = df[ocr_col].fillna('').astype(str)\n","\n","    feat = {}\n","    # Caption 統計\n","    feat['cap_len']       = caps.apply(lambda s: len(s))\n","    feat['cap_hashtags']  = caps.apply(lambda s: len(re.findall(r'#\\w+', s)))\n","    feat['cap_mentions']  = caps.apply(lambda s: len(re.findall(r'@\\w+', s)))\n","    feat['cap_digits']    = caps.apply(lambda s: len(re.findall(r'\\d', s)))\n","    feat['cap_bang']      = caps.apply(lambda s: s.count('!'))\n","    feat['cap_qmark']     = caps.apply(lambda s: s.count('?'))\n","    feat['cap_emoji']     = caps.apply(count_emojis)\n","    feat['cap_promo']     = caps.apply(lambda s: has_any(s, PROMO_WORDS))\n","    feat['cap_flavor']    = caps.apply(lambda s: has_any(s, FLAVOR_WORDS))\n","    feat['cap_health']    = caps.apply(lambda s: has_any(s, HEALTH_WORDS))\n","    feat['cap_cta']       = caps.apply(lambda s: has_any(s, CTA_WORDS))\n","\n","    # OCR 統計\n","    feat['ocr_len']       = ocrs.apply(lambda s: len(s))\n","    feat['ocr_digits']    = ocrs.apply(lambda s: len(re.findall(r'\\d', s)))\n","    feat['ocr_has_price'] = ocrs.apply(lambda s: int(re.search(PRICE_PAT, s.lower()) is not None))\n","    feat['ocr_has_pct']   = ocrs.apply(lambda s: int(re.search(PCT_PAT,   s.lower()) is not None))\n","    feat['ocr_promo']     = ocrs.apply(lambda s: has_any(s, PROMO_WORDS))\n","    feat['ocr_flavor']    = ocrs.apply(lambda s: has_any(s, FLAVOR_WORDS))\n","    feat['ocr_health']    = ocrs.apply(lambda s: has_any(s, HEALTH_WORDS))\n","    feat['ocr_cta']       = ocrs.apply(lambda s: has_any(s, CTA_WORDS))\n","\n","    # 時間特徵（使用 time_converted）\n","    t = pd.to_datetime(df[time_col], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","    hours = t.dt.hour.fillna(0).astype(int)\n","    feat['time_sin'] = np.sin(2*np.pi*hours/24)\n","    feat['time_cos'] = np.cos(2*np.pi*hours/24)\n","\n","    numeric_df = pd.DataFrame(feat, index=df.index)\n","    return numeric_df\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXhYbXFVPSbY","outputId":"4da79226-6b1c-40db-9eb4-64a99ca5a6ab"},"outputs":[{"name":"stdout","output_type":"stream","text":[" 找到嵌入快取，直接載入： /content/drive/MyDrive/project/output_split_73/cache/modal_embeddings_v2.npz\n"]}],"source":["# ======================================================\n","# ==== Cell 8（新版）產生三模態嵌入與 OCR ==========\n","# ====  (有整包快取：cap/ocr/img 一次存 npz)     ====\n","# ======================================================\n","\n","import os, hashlib\n","import numpy as np\n","from tqdm.auto import tqdm\n","\n","# === 影像嵌入快取（逐張圖片） ===\n","IMG_EMB_DIR = os.path.join(CACHE_DIR, 'img_emb_cache')\n","os.makedirs(IMG_EMB_DIR, exist_ok=True)\n","\n","def _img_key(path, model_tag=\"\"):\n","    \"\"\"\n","    用圖片路徑 + 模型身分產生唯一 key，避免換模型還沿用舊向量\n","    \"\"\"\n","    base = f\"{model_tag}|{path}\"\n","    return hashlib.sha1(base.encode('utf-8', 'ignore')).hexdigest()\n","\n","def get_img_emb_with_cache(pil_img, path_hint, model_tag=\"\"):\n","    \"\"\"\n","    單張圖片：如果已有 .npy 快取就直接讀，否則跑 embed_images_clip 再存\n","    \"\"\"\n","    key = _img_key(path_hint, model_tag)\n","    cache_path = os.path.join(IMG_EMB_DIR, f'{key}.npy')\n","    if os.path.exists(cache_path):\n","        return np.load(cache_path)\n","    arr = embed_images_clip([pil_img])  # [1, PROJ_DIM]\n","    vec = arr[0]\n","    np.save(cache_path, vec)\n","    return vec\n","\n","# 在 Cell 6 會設定 MODEL_BACKEND（'chinese-clip' 或 'openai-clip'）\n","MODEL_TAG = MODEL_BACKEND  # 也可以改成 MODEL_ID_CN / MODEL_ID_EN 更細\n","\n","def build_modal_embeddings(df, img_dir, split_name='train'):\n","    \"\"\"\n","    回傳：\n","      cap_emb: CLIP text embedding (caption)\n","      ocr_emb: CLIP text embedding (OCR 文字)\n","      img_emb: CLIP image embedding（多圖平均）\n","      ocr_texts: OCR 後的文字（list[str]）\n","      rel_lists : 每列的相對路徑清單\n","    \"\"\"\n","    rel_lists = df['rel_img_paths'].apply(parse_rel_img_paths).tolist()\n","\n","    # 1) OCR to text（有快取：ocr_post 會先看 CACHE_DIR/ocr_*.json）\n","    ocr_texts = []\n","    for i, rels in tqdm(list(enumerate(rel_lists)),\n","                        desc=f'OCR {split_name}', total=len(rel_lists)):\n","      row_idx = df.index[i]  # 用原始 row index 當鍵\n","      key = f\"v2_{split_name}_{row_idx}\"\n","      ocr_texts.append(ocr_post(rels, img_dir, key))\n","\n","    # 2) Caption / OCR embeddings（用 CLIP text encoder）\n","    cap_texts = df[caption_col].fillna('').astype(str).tolist()\n","    # 保險版：一次可能很多句，所以用 *_safe 包一層\n","    cap_emb = embed_text_clip_safe(cap_texts)\n","    ocr_emb = embed_text_clip_safe(ocr_texts)\n","\n","    # 3) Image embeddings（逐張圖片快取，最後平均）\n","    img_embs = []\n","    for rels in tqdm(rel_lists,\n","                     desc=f'Image emb {split_name}', total=len(rel_lists)):\n","        vecs = []\n","        for rp in rels[:IMG_MAX_IMAGES]:\n","            p = os.path.join(img_dir, rp)\n","            if not os.path.exists(p):\n","                # 有些路徑是子資料夾名，試著只用檔名兜一下\n","                p2 = os.path.join(img_dir, os.path.basename(rp))\n","                if os.path.exists(p2):\n","                    p = p2\n","                else:\n","                    continue\n","            im = load_image_for_clip(p)\n","            if im is None:\n","                continue\n","            v = get_img_emb_with_cache(im, p, MODEL_TAG)\n","            vecs.append(v)\n","\n","        if len(vecs) == 0:\n","            mean_vec = np.zeros((PROJ_DIM,), dtype=np.float32)\n","        else:\n","            arr = np.vstack(vecs)\n","            mean_vec = arr.mean(axis=0)\n","            mean_vec = mean_vec / (np.linalg.norm(mean_vec) + 1e-9)  # L2 normalize\n","\n","        img_embs.append(mean_vec.astype(np.float32))\n","\n","    img_emb = np.vstack(img_embs)\n","\n","    return cap_emb, ocr_emb, img_emb, ocr_texts, rel_lists\n","\n","\n","# === 整包嵌入快取（訓練 + 測試） ===\n","EMB_CACHE_FILE = os.path.join(CACHE_DIR, 'modal_embeddings_v2.npz')\n","\n","if os.path.exists(EMB_CACHE_FILE):\n","    print(\" 找到嵌入快取，直接載入：\", EMB_CACHE_FILE)\n","    data = np.load(EMB_CACHE_FILE, allow_pickle=True)\n","\n","    cap_train = data['cap_train']\n","    ocr_train = data['ocr_train']\n","    img_train = data['img_train']\n","\n","    cap_test  = data['cap_test']\n","    ocr_test  = data['ocr_test']\n","    img_test  = data['img_test']\n","\n","    ocr_text_train = data['ocr_text_train'].tolist()\n","    ocr_text_test  = data['ocr_text_test'].tolist()\n","\n","else:\n","    print(\" 找不到嵌入快取，開始重算 CLIP 嵌入與 OCR …\")\n","    cap_train, ocr_train, img_train, ocr_text_train, rels_train = \\\n","        build_modal_embeddings(train, IMG_TRAIN_DIR, 'train')\n","    cap_test,  ocr_test,  img_test,  ocr_text_test,  rels_test  = \\\n","        build_modal_embeddings(test,  IMG_TEST_DIR,  'test')\n","\n","    # 存成 npz，之後重開 runtime 也可以直接載入\n","    np.savez(\n","        EMB_CACHE_FILE,\n","        cap_train=cap_train, ocr_train=ocr_train, img_train=img_train,\n","        cap_test=cap_test,   ocr_test=ocr_test,   img_test=img_test,\n","        ocr_text_train=np.array(ocr_text_train, dtype=object),\n","        ocr_text_test=np.array(ocr_text_test, dtype=object),\n","    )\n","    print(\" 已寫入嵌入快取：\", EMB_CACHE_FILE)\n","\n","# 把 OCR 文字塞回 DataFrame，之後 Cell 9 / 13 會用到\n","train['ocr_text'] = ocr_text_train\n","test['ocr_text']  = ocr_text_test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TtieuxK8PT7Y"},"outputs":[],"source":["# =================================================\n","# ==== Cell 9: 數值型 Metadata（z-score + 縮放） ====\n","# =================================================\n","numeric_train = build_numeric_features(train, caption_col, 'ocr_text', time_col=time_col)\n","numeric_test  = build_numeric_features(test,  caption_col, 'ocr_text', time_col=time_col)\n","\n","scaler = StandardScaler()\n","numeric_train_z = pd.DataFrame(scaler.fit_transform(numeric_train), columns=numeric_train.columns, index=train.index)\n","numeric_test_z  = pd.DataFrame(scaler.transform(numeric_test),  columns=numeric_test.columns,  index=test.index)\n","\n","# 將數值型向量轉為 numpy\n","num_train_vec = numeric_train_z.values.astype(np.float32)\n","num_test_vec  = numeric_test_z.values.astype(np.float32)\n","\n","# --- 縮放 numeric 區塊，避免壓過語意 ---\n","# 先計算語意區塊（cap+ocr+img）的平均 L2 範數，再把 numeric 區塊縮到語意的一定比例（例如 0.5）\n","sem_train_vec = np.hstack([cap_train, ocr_train, img_train])\n","sem_norm_mean = np.mean(np.linalg.norm(sem_train_vec, axis=1))\n","num_norm_mean = np.mean(np.linalg.norm(num_train_vec, axis=1) + 1e-9)\n","delta = 0.5 * (sem_norm_mean / (num_norm_mean + 1e-9))  # 讓 numeric 平均範數約為語意的一半\n","num_train_vec_scaled = num_train_vec * delta\n","num_test_vec_scaled  = num_test_vec  * delta\n","\n","# ---- 構成 joint_emb（全模態用於 Phase 1 子模態切分）----\n","# Text 模態 = cap_emb + ocr_emb\n","text_train_vec = np.hstack([cap_train, ocr_train]).astype(np.float32)\n","text_test_vec  = np.hstack([cap_test,  ocr_test ]).astype(np.float32)\n","# Image 模態\n","image_train_vec = img_train.astype(np.float32)\n","image_test_vec  = img_test.astype(np.float32)\n","# Meta 模態（純 numeric）\n","meta_train_vec  = num_train_vec_scaled.astype(np.float32)\n","meta_test_vec   = num_test_vec_scaled.astype(np.float32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eK7aJuEHPVkw"},"outputs":[],"source":["# =======================================\n","# ==== Cell 10: Novelty / Diversity  ====\n","# =======================================\n","def fit_anchors(X, k=K_CLUSTERS, random_state=SEED):\n","    \"\"\"\n","    訓練期建立 K-means 中心（錨點）\n","    這裡使用 MiniBatchKMeans 以支援 sample_weight（較節省記憶體）\n","    \"\"\"\n","    km = MiniBatchKMeans(n_clusters=k, random_state=random_state, batch_size=512, n_init='auto')\n","    km.fit(X)\n","    centers = km.cluster_centers_\n","    # 對中心做 L2 normalize（讓 cosine 更穩）\n","    centers = centers / (np.linalg.norm(centers, axis=1, keepdims=True) + 1e-9)\n","    return centers\n","\n","def compute_novelty(X, centers):\n","    \"\"\"\n","    Novelty = 1 - max cosine similarity to centers\n","    \"\"\"\n","    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n","    sims = np.dot(Xn, centers.T)  # [n, k]\n","    max_sim = sims.max(axis=1)\n","    nov_raw = 1.0 - max_sim\n","    return nov_raw, sims  # sims 備用給 diversity\n","\n","def compute_diversity_from_sims(sims, tau=TAU):\n","    \"\"\"\n","    Diversity = normalized entropy of softmax(sim/tau)\n","    \"\"\"\n","    probs = np.vstack([softmax(row, tau) for row in sims])  # [n, k]\n","    # entropy\n","    ent = -(probs * (np.log(probs + 1e-9))).sum(axis=1)\n","    # normalize by log(k)\n","    k = sims.shape[1]\n","    ent_norm = ent / (math.log(k) + 1e-9)\n","    return ent_norm\n","\n","def minmax_fit(x):\n","    mn, mx = float(np.min(x)), float(np.max(x))\n","    if mx - mn < 1e-9:\n","        return mn, mx\n","    return mn, mx\n","\n","def minmax_transform(x, mn, mx):\n","    if mx - mn < 1e-9:\n","        return np.zeros_like(x, dtype=np.float32)\n","    return ((x - mn) / (mx - mn)).astype(np.float32)\n","\n","def compute_modality_scores(train_vec, test_vec, k=K_CLUSTERS, tau=TAU):\n","    # anchors\n","    centers = fit_anchors(train_vec, k=k)\n","    # novelty\n","    nov_tr_raw, sims_tr = compute_novelty(train_vec, centers)\n","    nov_te_raw, sims_te = compute_novelty(test_vec,  centers)\n","    # min-max（訓練集為準，離群納入）\n","    mn, mx = minmax_fit(nov_tr_raw)\n","    nov_tr = minmax_transform(nov_tr_raw, mn, mx)\n","    nov_te = minmax_transform(nov_te_raw, mn, mx)\n","    # diversity\n","    div_tr = compute_diversity_from_sims(sims_tr, tau=tau).astype(np.float32)\n","    div_te = compute_diversity_from_sims(sims_te, tau=tau).astype(np.float32)\n","    return {\n","        'centers': centers,\n","        'nov_tr': nov_tr, 'nov_te': nov_te,\n","        'div_tr': div_tr, 'div_te': div_te,\n","        'nov_min': mn, 'nov_max': mx\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oy79jNyXPXDA"},"outputs":[],"source":["# =========================================\n","# ==== Cell 11: Phase 1（各模態學權重）====\n","# =========================================\n","# 目標變數 y：這一版先只用 likes\n","LAMBDA_COMMENT = 0.0   # =0 -> 只用 likes；若未來要加留言，可改成 5.0 之類的值\n","DELTA = 0.01\n","GAMMA = 1.0\n","\n","def compute_y(likes, comments, followers,\n","              lambda_c=LAMBDA_COMMENT, delta=DELTA, gamma=GAMMA):\n","    \"\"\"目前：y = (likes + lambda_c * comments) / (followers + delta)^gamma\"\"\"\n","    likes = likes.astype(float)\n","    comments = comments.astype(float)\n","    followers = followers.astype(float)\n","    return (likes + lambda_c * comments) / ((followers + delta) ** gamma)\n","\n","\n","train['y'] = compute_y(\n","    train['count_like'].fillna(0).to_numpy(),\n","    train['count_comment'].fillna(0).to_numpy(),\n","    train['followers'].fillna(0).to_numpy(),\n",")\n","test['y'] = compute_y(\n","    test['count_like'].fillna(0).to_numpy(),\n","    test['count_comment'].fillna(0).to_numpy(),\n","    test['followers'].fillna(0).to_numpy(),\n",")\n","\n","\n","# brand 反比權重（用於回歸訓練）\n","brand_counts = train[brand_col].value_counts().to_dict()\n","train_weights = train[brand_col].map(lambda b: 1.0 / math.sqrt(brand_counts.get(b,1))).to_numpy()\n","\n","# late-entry 標記（訓練為 0 篇、測試 > 0 篇）\n","train_brands = set(train[brand_col].unique().tolist())\n","test_brands  = set(test[brand_col].unique().tolist())\n","late_entry_brands = sorted(list(test_brands - train_brands))\n","test['is_late_entry_brand'] = test[brand_col].isin(late_entry_brands).astype(int)\n","\n","def learn_wN_wD(nov_tr, div_tr, y_tr, sample_weight=None):\n","    X = np.vstack([nov_tr, div_tr]).T.astype(np.float32)\n","    lr = LinearRegression()\n","    if sample_weight is not None:\n","        lr.fit(X, y_tr, sample_weight=sample_weight)\n","    else:\n","        lr.fit(X, y_tr)\n","    beta = lr.coef_.astype(np.float32)  # [beta_N, beta_D]\n","    # 截斷負值，避免解釋衝突；全為 0 時設為均分\n","    beta = np.maximum(beta, 0.0)\n","    if beta.sum() < 1e-9:\n","        wN, wD = 0.5, 0.5\n","    else:\n","        wN, wD = (beta / (beta.sum()+1e-9)).tolist()\n","    return float(wN), float(wD), lr\n","\n","def phase1_per_modality(train_vec, test_vec, y_tr, sample_weight=None, name='text'):\n","    pack = compute_modality_scores(train_vec, test_vec, k=K_CLUSTERS, tau=TAU)\n","    wN, wD, lr = learn_wN_wD(pack['nov_tr'], pack['div_tr'], y_tr, sample_weight=sample_weight)\n","    # Distinctiveness Score（DS）與 風險 ATI\n","    DS_tr = (wN*pack['nov_tr'] + wD*pack['div_tr']).astype(np.float32)\n","    DS_te = (wN*pack['nov_te'] + wD*pack['div_te']).astype(np.float32)\n","    ATI_tr = 100.0*(1.0 - DS_tr)\n","    ATI_te = 100.0*(1.0 - DS_te)\n","    return {\n","        'name': name,\n","        'centers': pack['centers'],\n","        'nov_tr': pack['nov_tr'], 'nov_te': pack['nov_te'],\n","        'div_tr': pack['div_tr'], 'div_te': pack['div_te'],\n","        'wN': wN, 'wD': wD,\n","        'DS_tr': DS_tr, 'DS_te': DS_te,\n","        'ATI_tr': ATI_tr, 'ATI_te': ATI_te,\n","        'nov_min': pack['nov_min'], 'nov_max': pack['nov_max']\n","    }\n","\n","# 二個模態跑 Phase 1\n","phase1_text  = phase1_per_modality(text_train_vec,  text_test_vec,  train['y'].to_numpy(), sample_weight=train_weights, name='text')\n","phase1_image = phase1_per_modality(image_train_vec, image_test_vec, train['y'].to_numpy(), sample_weight=train_weights, name='image')\n","#phase1_meta  = phase1_per_modality(meta_train_vec,  meta_test_vec,  train['y'].to_numpy(), sample_weight=train_weights, name='meta')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLlPoKhnPYkI","outputId":"b2025213-7a00-4dd2-b3f9-5142d8f5127d"},"outputs":[{"data":{"text/plain":["{'phase1_text_wN': 0.5,\n"," 'phase1_text_wD': 0.5,\n"," 'phase1_image_wN': 0.3501806855201721,\n"," 'phase1_image_wD': 0.6498191952705383,\n"," 'phase2_v_text': 0.0,\n"," 'phase2_v_image': 1.0}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# =========================================\n","# ==== Cell 12: Phase 2（二階段合成） ======\n","# =========================================\n","# 這一版：最終 ATI 只用 text + image，metadata 不參與 Phase 2。\n","\n","# Train 組合（只用 text & image 的 DS）\n","DS_text_tr  = phase1_text['DS_tr']\n","DS_image_tr = phase1_image['DS_tr']\n","X_tr = np.vstack([DS_text_tr, DS_image_tr]).T.astype(np.float32)\n","y_tr = train['y'].to_numpy()\n","\n","# 以 brand 反比權重做二階段回歸\n","lr2 = LinearRegression()\n","lr2.fit(X_tr, y_tr, sample_weight=train_weights)\n","coef2 = np.maximum(lr2.coef_.astype(np.float32), 0.0)\n","if coef2.sum() < 1e-9:\n","    v = np.array([0.5, 0.5], dtype=np.float32)\n","else:\n","    v = coef2 / coef2.sum()\n","\n","# Test 組合\n","DS_text_te  = phase1_text['DS_te']\n","DS_image_te = phase1_image['DS_te']\n","\n","DS_final_tr = (v[0]*DS_text_tr + v[1]*DS_image_tr).astype(np.float32)\n","DS_final_te = (v[0]*DS_text_te + v[1]*DS_image_te).astype(np.float32)\n","\n","ATI_final_tr = 100.0*(1.0 - DS_final_tr)\n","ATI_final_te = 100.0*(1.0 - DS_final_te)\n","\n","# 保存權重（Phase 1 二模態；Phase 2 只有 text/image）\n","weights_summary = {\n","    'phase1_text_wN':  phase1_text['wN'],  'phase1_text_wD':  phase1_text['wD'],\n","    'phase1_image_wN': phase1_image['wN'], 'phase1_image_wD': phase1_image['wD'],\n","    #'phase1_meta_wN':  phase1_meta['wN'],  'phase1_meta_wD':  phase1_meta['wD'],\n","    'phase2_v_text': float(v[0]),\n","    'phase2_v_image': float(v[1]),\n","}\n","weights_summary\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUfNr26mPbvg","outputId":"2af7f183-3886-446e-ebe0-ee1843bbf390"},"outputs":[{"data":{"text/plain":["('/content/drive/MyDrive/project/output_split_73/outputs/ati_train_per_post.csv',\n"," '/content/drive/MyDrive/project/output_split_73/outputs/ati_test_per_post.csv',\n"," '/content/drive/MyDrive/project/output_split_73/outputs/ati_test_brand_agg.csv')"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# =========================================\n","# ==== Cell 13: 輸出 per-post & per-brand ===\n","# =========================================\n","# Per-post（train/test）\n","post_train_out = train[[brand_col, 'count_like','count_comment','followers']].copy()\n","post_test_out  = test[[brand_col,  'count_like','count_comment','followers','is_late_entry_brand']].copy()\n","\n","post_train_out['y'] = train['y'].values\n","post_test_out['y']  = test['y'].values\n","\n","# 各模態 N/D 與 ATI\n","def add_phase1_outputs(df_out, phase1, split='tr'):\n","    df_out[f'{phase1[\"name\"]}_nov'] = phase1[f'nov_{split}']\n","    df_out[f'{phase1[\"name\"]}_div'] = phase1[f'div_{split}']\n","    df_out[f'{phase1[\"name\"]}_DS']  = phase1[f'DS_{split}']\n","    df_out[f'{phase1[\"name\"]}_ATI'] = phase1[f'ATI_{split}']\n","\n","add_phase1_outputs(post_train_out, phase1_text,  'tr')\n","add_phase1_outputs(post_train_out, phase1_image, 'tr')\n","#add_phase1_outputs(post_train_out, phase1_meta,  'tr')\n","post_train_out['ATI_final'] = ATI_final_tr\n","post_train_out['DS_final']  = DS_final_tr\n","\n","add_phase1_outputs(post_test_out, phase1_text,  'te')\n","add_phase1_outputs(post_test_out, phase1_image, 'te')\n","#add_phase1_outputs(post_test_out, phase1_meta,  'te')\n","post_test_out['ATI_final'] = ATI_final_te\n","post_test_out['DS_final']  = DS_final_te\n","\n","# 加入文字欄位（方便檢視）\n","post_train_out['caption']  = train[caption_col]\n","post_train_out['ocr_text'] = train['ocr_text']\n","post_test_out['caption']   = test[caption_col]\n","post_test_out['ocr_text']  = test['ocr_text']\n","\n","# 存檔\n","train_csv_out = os.path.join(OUT_DIR, 'ati_train_per_post.csv')\n","test_csv_out  = os.path.join(OUT_DIR, 'ati_test_per_post.csv')\n","post_train_out.to_csv(train_csv_out, index=False)\n","post_test_out.to_csv(test_csv_out, index=False)\n","\n","# 品牌彙總（測試期）\n","brand_test_agg = post_test_out.groupby(brand_col).agg(\n","    n_posts=('ATI_final','size'),\n","    ATI_final_mean=('ATI_final','mean'),\n","    DS_final_mean=('DS_final','mean'),\n","    y_mean=('y','mean'),\n","    late_entry_brand=('is_late_entry_brand','max')\n",").reset_index()\n","\n","brand_csv_out = os.path.join(OUT_DIR, 'ati_test_brand_agg.csv')\n","brand_test_agg.to_csv(brand_csv_out, index=False)\n","\n","train_csv_out, test_csv_out, brand_csv_out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FC1cxd_ePcGQ","outputId":"529f1b7a-a88c-42c2-b26b-018ed0846bbd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Phase 1: Text  vs y (train/test) = (-0.027525630402464325, 0.27012210174559753) (0.04957936607304481, 0.2291837003027624)\n","Phase 1: Image vs y (train/test) = (0.006513370805531809, 0.7941667516083617) (-0.02398179751631884, 0.5609950845614633)\n","Phase 2: DS_final vs y (train/test) = (0.006513370805531809, 0.7941667516083617) (-0.02398179751631884, 0.5609950845614633)\n","輸出檔： /content/drive/MyDrive/project/output_split_73/outputs/ati_train_per_post.csv /content/drive/MyDrive/project/output_split_73/outputs/ati_test_per_post.csv /content/drive/MyDrive/project/output_split_73/outputs/ati_test_brand_agg.csv\n"]}],"source":["# =========================================\n","# ==== Cell 14: 簡單檢查 ==========\n","# =========================================\n","from scipy.stats import spearmanr\n","\n","def spearman_safe(a, b):\n","    try:\n","        r, p = spearmanr(a, b)\n","        return float(r), float(p)\n","    except Exception:\n","        return np.nan, np.nan\n","\n","print(\"Phase 1: Text  vs y (train/test) =\",\n","      spearman_safe(phase1_text['DS_tr'], train['y']),\n","      spearman_safe(phase1_text['DS_te'], test['y']))\n","print(\"Phase 1: Image vs y (train/test) =\",\n","      spearman_safe(phase1_image['DS_tr'], train['y']),\n","      spearman_safe(phase1_image['DS_te'], test['y']))\n","\n","print(\"Phase 2: DS_final vs y (train/test) =\",\n","      spearman_safe(DS_final_tr, train['y']),\n","      spearman_safe(DS_final_te, test['y'])\n","     )\n","\n","print(\"輸出檔：\", train_csv_out, test_csv_out, brand_csv_out)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GH1iRPoZ5Dfw","outputId":"7fbd1684-f265-42f1-9074-d33515ba72ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["DS_final vs y: Spearman rho=-0.024, p=5.61e-01\n","ATI_final vs y: Spearman rho=0.024, p=5.61e-01\n"]}],"source":["#(a) Spearman 相關（方向 sanity check）\n","import pandas as pd\n","from scipy.stats import spearmanr\n","import os\n","\n","df_te = pd.read_csv(os.path.join(BASE_DIR, 'outputs', 'ati_test_per_post.csv'))\n","\n","pairs = [\n","    ('DS_text','y'), ('DS_image','y'), ('DS_meta','y'), ('DS_final','y'),\n","    ('ATI_text','y'), ('ATI_image','y'), ('ATI_meta','y'), ('ATI_final','y'),\n","]\n","for a,b in pairs:\n","    if a in df_te.columns and b in df_te.columns:\n","        r,p = spearmanr(df_te[a], df_te[b], nan_policy='omit')\n","        print(f\"{a} vs {b}: Spearman rho={r:.3f}, p={p:.2e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEzK22Op5LiD","outputId":"47fe2308-5cfb-4e0f-dc5d-fc533664caa2"},"outputs":[{"name":"stdout","output_type":"stream","text":["   decile  count      mean    median\n","0       0     59  0.016946  0.005496\n","1       1     59  0.013231  0.004566\n","2       2     59  0.017356  0.007003\n","3       3     59  0.023300  0.008561\n","4       4     59  0.016517  0.007508\n","5       5     59  0.012656  0.004152\n","6       6     59  0.015800  0.005349\n","7       7     59  0.019788  0.006500\n","8       8     59  0.023197  0.006756\n","9       9     59  0.062604  0.006980\n"]}],"source":["#(b) 分箱（deciles）看單調性\n","import numpy as np\n","\n","# 以 ATI_final 為主，把 test 貼文分成 10 等分\n","df_te = df_te.dropna(subset=['ATI_final', 'y']).copy()\n","df_te['decile'] = pd.qcut(df_te['ATI_final'], 10, labels=False, duplicates='drop')\n","dec = df_te.groupby('decile')['y'].agg(['count','mean','median']).reset_index()\n","print(dec)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"VoIGXx485N1w","outputId":"85dbabb7-8d24-4970-8df7-3e65c6f8ee18"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 10 brands by y_mean\n","                  brand  n_posts  ATI_final_mean  DS_final_mean    y_mean  \\\n","54      wanpotea.com.tw        3       31.840288       0.681597  1.045332   \n","16           hanlin.tea        6       28.870780       0.711292  0.173868   \n","5   chatime.tw_official        9       27.681100       0.723189  0.134456   \n","6            chosen.tea        6       34.162083       0.658379  0.072762   \n","46        thefarfarfarm        2       32.488330       0.675117  0.041393   \n","22           macu2008tw       34       25.850254       0.741498  0.039210   \n","28           mr.wish_tw       12       21.883200       0.781168  0.039014   \n","61          youintw2021        9       24.560898       0.754391  0.031866   \n","2            chaffee_tw        1       26.612680       0.733873  0.031033   \n","20          liketeashop        9       26.814938       0.731851  0.029702   \n","\n","    late_entry_brand  \n","54                 0  \n","16                 0  \n","5                  0  \n","6                  0  \n","46                 0  \n","22                 0  \n","28                 0  \n","61                 0  \n","2                  0  \n","20                 0  \n","\n","Lowest ATI_final_mean (風險最低) brands\n","                  brand  n_posts  ATI_final_mean  DS_final_mean    y_mean  \\\n","43       teamagichand00        1       17.432434       0.825676  0.002900   \n","38            t4_global        1       17.689627       0.823104  0.012783   \n","40   taro_yuan.official        2       18.700188       0.812998  0.003479   \n","1           blackteabus        7       19.634700       0.803653  0.009252   \n","4             chanungtw        9       19.711884       0.802881  0.007097   \n","59           y.j_coffee        6       20.227560       0.797724  0.007596   \n","52       ugtea_official       14       21.181010       0.788190  0.017895   \n","13  dragonhorn.official        2       21.376358       0.786236  0.016882   \n","8          chunshuitang       11       21.763441       0.782366  0.017512   \n","50    truewin_lucteaday        7       21.879990       0.781200  0.001275   \n","\n","    late_entry_brand  \n","43                 0  \n","38                 0  \n","40                 0  \n","1                  0  \n","4                  0  \n","59                 0  \n","52                 0  \n","13                 0  \n","8                  0  \n","50                 0  \n","\n","Most novel posts (lowest ATI_final):\n"]},{"ename":"KeyError","evalue":"\"['time_converted', 'sum'] not in index\"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4122869684.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'brand'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'time_converted'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ocr_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DS_final'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ATI_final'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMost novel posts (lowest ATI_final):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ATI_final'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMost average-like posts (highest ATI_final):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['time_converted', 'sum'] not in index\""]}],"source":["#(c) 品牌層級總表與極值範例\n","brand = pd.read_csv(os.path.join(BASE_DIR, 'outputs', 'ati_test_brand_agg.csv'))\n","print(\"Top 10 brands by y_mean\")\n","print(brand.sort_values('y_mean', ascending=False).head(10))\n","\n","print(\"\\nLowest ATI_final_mean (風險最低) brands\")\n","print(brand.sort_values('ATI_final_mean', ascending=True).head(10))\n","\n","# 貼文範例：最「新穎」（ATI_final 最低）與最「平均」（ATI_final 最高）\n","cols = ['brand','time_converted','sum','ocr_text','y','DS_final','ATI_final']\n","print(\"\\nMost novel posts (lowest ATI_final):\")\n","print(df_te.sort_values('ATI_final', ascending=True).head(8)[cols])\n","\n","print(\"\\nMost average-like posts (highest ATI_final):\")\n","print(df_te.sort_values('ATI_final', ascending=False).head(8)[cols])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnQPCqEW5VGs"},"outputs":[],"source":["# (d)把關鍵圖表存檔（無 seaborn；matplotlib 單色）\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# 1) 散點：ATI_final vs y\n","plt.figure()\n","plt.scatter(df_te['ATI_final'], df_te['y'], s=8, alpha=0.5)\n","plt.xlabel('ATI_final (lower is better)')\n","plt.ylabel('Engagement y')\n","plt.title('ATI_final vs y (test)')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.savefig(os.path.join(BASE_DIR,'outputs','plot_ati_final_vs_y.png'))\n","\n","# 2) 分箱柱狀：ATI deciles vs y mean\n","plt.figure()\n","plt.bar(dec['decile'].astype(str), dec['mean'])\n","plt.xlabel('ATI_final decile (low→high)')\n","plt.ylabel('mean(y)')\n","plt.title('Decile plot (test)')\n","plt.tight_layout()\n","plt.savefig(os.path.join(BASE_DIR,'outputs','plot_deciles.png'))\n","print(\"Saved plots to outputs/\")\n"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"038be2dac2f540019a2fe530fbb7db02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f3a8c4eb3ff481da7833c0a9833e4cc","placeholder":"​","style":"IPY_MODEL_fd1f9d1334d444ffa3398567b0e951dc","value":"pytorch_model.bin: 100%"}},"11545df95e6143f5b77f9866f2bf98cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2089125947b04a3a928b1593373c8edc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3efe8d9aaedf4db482c6c9a266e8e5ae","IPY_MODEL_993e33d3fde9441bad7120a9a2eefa15","IPY_MODEL_c44b302036b7443f83827aa3a3452e8d"],"layout":"IPY_MODEL_76ed02c4224244a39c36f6f47470cafc"}},"3226c05f59fe4e268de0f4b444b8fbce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be0eb51ff0b84e9cb9f4f94a6458a10e","placeholder":"​","style":"IPY_MODEL_c50d2d48d9d649dfb70a533bcace8e6b","value":" 753M/753M [00:04&lt;00:00, 338MB/s]"}},"3df5933cd5254398929bf5a15daae1f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3efe8d9aaedf4db482c6c9a266e8e5ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0bb7002d9974f68a7092939e0addb05","placeholder":"​","style":"IPY_MODEL_cb1e961dfdf4430fb6ef430154ee3766","value":"config.json: "}},"4190d8e52c0742fcb036b80ec0e84c32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43a4c70835c249e58495f7f326c6c188":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf4f6e30f1914a7483407b7e924a39d4","IPY_MODEL_dac3d7c3cb7c4a3db01ef4f65e8be6b5","IPY_MODEL_f1827ebd3e54406a9dbf9475f8ef1b6a"],"layout":"IPY_MODEL_c13ff9f13f43415a839b32c49c471b46"}},"45e628433d87458eb822b46a821532df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8382eb04b6e485c98fb3a984559ddd9","placeholder":"​","style":"IPY_MODEL_cae64c9b45ab4f6999936fc6c96007a7","value":" 110k/? [00:00&lt;00:00, 10.8MB/s]"}},"531198654d8d4859ad5a3e6ccedaef26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ce1d004fed04c768e83990a64bfb9a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e5d4a0f136f40a3acd194272b119e18":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7d360c487b74e8ab5ea132bb683e7cc","placeholder":"​","style":"IPY_MODEL_b558c921c1ac48f0849701b52fe622bc","value":"vocab.txt: "}},"5f3a8c4eb3ff481da7833c0a9833e4cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f91453e4fd0472bb3e377802ef5f387":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"621e5f5f44bf47e79531e2519a40a8d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63ed3dcfcad84ab18ba4be96ee2f05eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e416ff5a73564220bb05ad0457d543dd","max":753177983,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3df5933cd5254398929bf5a15daae1f6","value":753177983}},"76ed02c4224244a39c36f6f47470cafc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80806f2b8d8d4b779755fe519a8f573e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81805c5f925143d79b4b3d001fb0732f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92f6f542fbee4b0b9b893274f51061b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9355e2a89610491285fb3c484914fd93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_038be2dac2f540019a2fe530fbb7db02","IPY_MODEL_63ed3dcfcad84ab18ba4be96ee2f05eb","IPY_MODEL_3226c05f59fe4e268de0f4b444b8fbce"],"layout":"IPY_MODEL_621e5f5f44bf47e79531e2519a40a8d7"}},"993e33d3fde9441bad7120a9a2eefa15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f91453e4fd0472bb3e377802ef5f387","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ee469f48de64a34900378f7c0e4a020","value":1}},"9ee469f48de64a34900378f7c0e4a020":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8382eb04b6e485c98fb3a984559ddd9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b558c921c1ac48f0849701b52fe622bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd795c3dbc7d4f1ebb97588b54e7e825":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be0eb51ff0b84e9cb9f4f94a6458a10e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf4f6e30f1914a7483407b7e924a39d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81805c5f925143d79b4b3d001fb0732f","placeholder":"​","style":"IPY_MODEL_92f6f542fbee4b0b9b893274f51061b0","value":"preprocessor_config.json: 100%"}},"c13ff9f13f43415a839b32c49c471b46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c44b302036b7443f83827aa3a3452e8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ce1d004fed04c768e83990a64bfb9a5","placeholder":"​","style":"IPY_MODEL_80806f2b8d8d4b779755fe519a8f573e","value":" 3.01k/? [00:00&lt;00:00, 355kB/s]"}},"c50d2d48d9d649dfb70a533bcace8e6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cae64c9b45ab4f6999936fc6c96007a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb1e961dfdf4430fb6ef430154ee3766":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdead9ef4b764759a2cb0a244f9a911c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d69579ba21a8435f92d8059018168876":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d81a1222838d4f38afd715a45a7f0494":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"dac3d7c3cb7c4a3db01ef4f65e8be6b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd795c3dbc7d4f1ebb97588b54e7e825","max":342,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4190d8e52c0742fcb036b80ec0e84c32","value":342}},"e0bb7002d9974f68a7092939e0addb05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e416ff5a73564220bb05ad0457d543dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eec1675c80234329a09a9a806bdfd77f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d81a1222838d4f38afd715a45a7f0494","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_531198654d8d4859ad5a3e6ccedaef26","value":1}},"f1827ebd3e54406a9dbf9475f8ef1b6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdead9ef4b764759a2cb0a244f9a911c","placeholder":"​","style":"IPY_MODEL_11545df95e6143f5b77f9866f2bf98cd","value":" 342/342 [00:00&lt;00:00, 43.8kB/s]"}},"f7d360c487b74e8ab5ea132bb683e7cc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd1f9d1334d444ffa3398567b0e951dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffd1cf5150d1407fa39effba2627a579":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e5d4a0f136f40a3acd194272b119e18","IPY_MODEL_eec1675c80234329a09a9a806bdfd77f","IPY_MODEL_45e628433d87458eb822b46a821532df"],"layout":"IPY_MODEL_d69579ba21a8435f92d8059018168876"}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}